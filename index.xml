<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Harvester Test Cases on Harvester manual test cases</title>
    <link>https://harvester.github.io/tests/</link>
    <description>Recent content in Harvester Test Cases on Harvester manual test cases</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://harvester.github.io/tests/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>01-Import existing Harvester clusters in Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/01-import-existing-harvester-in-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/01-import-existing-harvester-in-rancher/</guid>
      <description>This feature have been deprecated which already enhanced and merge to setup from harvester settings Please refer to 02-Integrate to Rancher from Harvester settings to test this feature
 Login rancher dashboard Navigate to Virtual Management Page Click import existing Copy the curl command  SSH to harvester master node (user: rancher) Execute the curl command to import harvester to rancher curl --insecure -sfL https://192.168.50.82/v3/import/{identifier}.yaml | kubectl apply -f - Run sudo chmod 775 /etc/rancher/rke2/rke2.</description>
    </item>
    
    <item>
      <title>02-Integrate to Rancher from Harvester settings (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/02-integrate-rancher-from-harvester-settings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/02-integrate-rancher-from-harvester-settings/</guid>
      <description>Environment setup  Install the latest rancher from docker command  $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6-head Create an one node harvester cluster Both harvester and rancher have internet connection  Verification Steps  Access rancher dashboard Open Virtualization Management page Import existing harvester Copy the registration url  Create image from URL (change folder date to latest) https://cloud-images.ubuntu.com/focal/20211122/focal-server-cloudimg-amd64.img Access harvester dashboard Edit cluster-registration-url in settings  Paste the registration url and save Back to rancher and wait for harvester imported in Rancher  Expected Results  Harvester can be imported in rancher dashboard with running status Can access harvester in virtual machine page Can create harvester cloud credential Can load harvester cloud credential while creating harvester  </description>
    </item>
    
    <item>
      <title>03-Manage VM in Downstream Harvster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/03-manage-vm-downstream-harvster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/03-manage-vm-downstream-harvster/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard
 Open harvester from Virtualization Management page Open Virtual Machine page Create a single instance virtual machine in Virtual Machines page Create multiple 3 instances virtual machines in Virtual Machines page Access and check virtual machine details Edit cpu, memory and network of one virtual machine Try Stop, Restart and Migrate virtual machine Try Clone virtual machine Try Delete virtual machine  Expected Results  Can create a single instance vm correctly Can create multiple instances vm correctly Can diaply all virtual machine information Can change cpu, memory and network and retart vm correctly Can Stop, Restart and Migrate virtual machine correctly Can Clone virtual machine correctly Can Delete virtual machine correctly  </description>
    </item>
    
    <item>
      <title>04-Manage Node in Downstream Harvster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/04-manage-host-downstream-harvster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/04-manage-host-downstream-harvster/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard
 Open harvester from Virtualization Management page Open Host page Access and check node details Edit node config, change network and add disk Try to Cordon and decordon node Enable and disable Maintenance mode  Expected Results  Can diaply all node&amp;rsquo;s information Can add disk to node correctly Can change network of node correctly Can Cordon and decordon node correctly Can enable and disable Maintenance mode  </description>
    </item>
    
    <item>
      <title>05-Manage Image in Downstream Harvster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/05-manage-image-volume-downstream-harvster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/05-manage-image-volume-downstream-harvster/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard
 Open harvester from Virtualization Management page Open Images page Create an image from URL Create an image from file Delete created images  Expected Results  Can create an image from URL Can create an image from file Can create an image from file Can delete created images correctly  </description>
    </item>
    
    <item>
      <title>06-Manage Network in Downstream Harvster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/06-manage-network-in-downstream-harvster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/06-manage-network-in-downstream-harvster/</guid>
      <description>Prerequisite: Harvester already imported to Rancher Dashboard
 Open harvester from Virtualization Management page Open Network page Create an new virtual network Create a new virtual machine using the new virtual network Delete a virtual network  Expected Results  Can create an new virtual network Create create a new virtual machine using the new virtual network Virtual machine can retrieve ip address Can delete a virtual network  </description>
    </item>
    
    <item>
      <title>07-Add and grant project-owner user to harvester (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/07-rbac-add-grant-project-owner-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/07-rbac-add-grant-project-owner-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-owner and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project   Search project-owner user Assign Owner role to it   Logout current user from Rancher Login with project-owner Open harvester from Virtualization Management page  Expected Results  Can create project-owner and set password Can assign Owner role to project-owner in default Can login correctly with project-owner Can manage all default project resources including host, virtual machines, volumes, VM and network  </description>
    </item>
    
    <item>
      <title>08-Add and grant project-readonly user to harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/08-rbac-add-grant-project-readonly-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/08-rbac-add-grant-project-readonly-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-readonly and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project   Search project-readonly user Assign Read Only role to it   Logout current user from Rancher Login with project-readonly Open harvester from Virtualization Management page  Expected Results  Can create project-readonly and set password Can assign Read Only role to project-readonly in default Can login correctly with project-readonly Can&amp;rsquo;t see Host page in harvester Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip;  </description>
    </item>
    
    <item>
      <title>09-Add and grant project-member user to harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/09-rbac-add-grant-project-member-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/09-rbac-add-grant-project-member-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-member and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project   Search project-member user Assign Member role to it   Logout current user from Rancher Login with project-member Open harvester from Virtualization Management page  Expected Results  Can create project-member and set password Can assign Member role to project-member in default Can login correctly with project-member Can&amp;rsquo;t see Host page in harvester Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip;  </description>
    </item>
    
    <item>
      <title>10-Add and grant project-custom user to harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/10-rbacadd-grant-project-custom-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/10-rbacadd-grant-project-custom-user-harvester/</guid>
      <description> Open Users &amp;amp; Authentication Click Users and Create Create user name project-custom and set password Select Standard User in the Global permission Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of default project   Search project-custom user Assign Custom role to it   Set Create Namespace, Manage Volumes and View Volumes Logout current user from Rancher Login with project-custom Open harvester from Virtualization Management page  Expected Results  Can create project-custom and set password Can assign Custom role to project-custom in default Can login correctly with project-custom Can do Create Namespace, Manage Volumes and View Volumes in default project  </description>
    </item>
    
    <item>
      <title>11-Create New Project in Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/11-create-project-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/11-create-project-harvester/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Click Create Project Set CPU and Memory limit in Resource Quotas   Change view to testProject only   Create some images Create some volumes Create a virtual machine  Expected Results  Can creat project correctly in Projects/Namespaces page Can create images correctly Can create volumes correctly Can create virtual machine correctly  </description>
    </item>
    
    <item>
      <title>12-Create New Namespace in Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/12-create-namespace-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/12-create-namespace-harvester/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Select the new project created in previous test case Click Create Namespace Set CPU and Memory limit in Container Resource Limit  Expected Results  Can creat new namepasce in correctly in create new project  </description>
    </item>
    
    <item>
      <title>13-Add and grant project-owner user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/13-rbac-add-grant-project-owner-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/13-rbac-add-grant-project-owner-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-owner user Assign Owner role to it Logout current user from Rancher Login with project-owner Open harvester from Virtualization Management page Change view to testProject only  Expected Results  Can assign Owner role to project-owner in testProject project Can manage all testProject project resources including host, virtual machines, volumes, VM and network  </description>
    </item>
    
    <item>
      <title>14-Add and grant project-readonly user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/14-rbac-add-grant-project-readonly-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/14-rbac-add-grant-project-readonly-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-readonly user Assign Read Only role to it Logout current user from Rancher Login with project-readonly Open harvester from Virtualization Management page Change view to testProject only  Expected Results  Can assign Read Only role to in testProject project Can login correctly with project-readonly Can&amp;rsquo;t see Host page in testProject only view Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; in testProject only view  </description>
    </item>
    
    <item>
      <title>15-Add and grant project-member user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/15-rbac-add-grant-project-member-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/15-rbac-add-grant-project-member-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-member user Assign Member role to it Logout current user from Rancher Login with project-member Open harvester from Virtualization Management page Change view to testProject only  Expected Results  Can assign Member role to project-member in testProject project Can login correctly with project-member Can&amp;rsquo;t see Host page in testProject project Can&amp;rsquo;t create or edit any resource including virtual machines, volumes, Images &amp;hellip; in testProject project  </description>
    </item>
    
    <item>
      <title>16-Add and grant project-custom user to custom project</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/16-rbac-add-grant-project-custom-user-custom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/16-rbac-add-grant-project-custom-user-custom/</guid>
      <description> Open harvester from Virtualization Management page Click Projects/Namespaces Edit config of testProject project Search project-custom user Assign Custom role to it Set Create Namespace, Manage Volumes and View Volumes Logout current user from Rancher Login with project-custom Open harvester from Virtualization Management page Change view to testProject only  Expected Results  Can assign Custom role to project-custom in testProject project Can login correctly with project-custom Can do Create Namespace, Manage Volumes and View Volumes in testProject project  </description>
    </item>
    
    <item>
      <title>17-Delete Imported Harvester Cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/17-delete-imported-harvester-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/17-delete-imported-harvester-cluster/</guid>
      <description> Finish 01-Import existing Harvester clusters in Rancher Open Virtualization Management page Delete already imported harvester  Expected Results  Can delete imported harvester correctly  </description>
    </item>
    
    <item>
      <title>18-Delete Failed Imported Harvester Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/18-delete-failed-imported-harvester-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/18-delete-failed-imported-harvester-cluster/</guid>
      <description> Make failure in 01-Import existing Harvester clusters in Rancher Open Virtualization Management page Delete already imported harvester  Expected Results  Can delete imported harvester correctly  </description>
    </item>
    
    <item>
      <title>19-Enable Harvester Node Driver</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/19-enable-harvester-node-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/19-enable-harvester-node-driver/</guid>
      <description>From Rancher v2.6.3-rc1, harvester node driver has already builtin racher We don&amp;rsquo;t need manual activate it
 Open Cluster Management Click Drivers page and navigate to Node Drivers tab Search harvester Check Harvester node driver is activated and mark as builtin  Expected Results  Status displayed Activated  </description>
    </item>
    
    <item>
      <title>20-Create RKE1 Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/20-create-rke1-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/20-create-rke1-kubernetes-cluster/</guid>
      <description>Click Cluster Management Click Cloud Credentials Click createa and select Harvester Input credential name Select existing cluster in the Imported Cluster list Click Create   Expand RKE1 Configuration Add Template in Node template Select Harvester Select created cloud credential created Select default namespace Select ubuntu image Select network: vlan1 Provide SSH User: ubuntu    Provide template name, click create   Open Cluster page, click Create</description>
    </item>
    
    <item>
      <title>21-Delete RKE1 Kubernetes Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/21-delete-rke1-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/21-delete-rke1-kubernetes-cluster/</guid>
      <description> Open Cluster Management Check provisioned RKE1 cluster Click Delete from menu  Expected Results  Can remove RKE1 Cluster and disapper on Cluster page RKE1 Cluster will be removed from rancher menu under explore cluster RKE1 virtual machine should be also be removed from Harvester  </description>
    </item>
    
    <item>
      <title>22-Create RKE2 Kubernetes Cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/22-create-rke2-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/22-create-rke2-kubernetes-cluster/</guid>
      <description> Click Cluster Management Click Cloud Credentials Click create and select Harvester Input credential name Select existing cluster in the Imported Cluster list Click Create   Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Click Create   Wait for RKE2 cluster provisioning complete (~20min)  Expected Results  Provision RKE2 cluster successfully with Running status   Can acccess RKE2 cluster to check all resources and services  </description>
    </item>
    
    <item>
      <title>23-Delete RKE2 Kubernetes Cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/23-delete-rke2-kubernetes-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/23-delete-rke2-kubernetes-cluster/</guid>
      <description> Open Cluster Management Check provisioned RKE2 cluster Click Delete from menu  Expected Results  Can remove RKE2 Cluster and disapper on Cluster page RKE2 Cluster will be removed from rancher menu under explore cluster RKE2 virtual machine should be also be removed from Harvester  </description>
    </item>
    
    <item>
      <title>24-Delete RKE1 Kubernetes Cluster in Provisioning</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/24-delete-rke1-kubernetes-cluster-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/24-delete-rke1-kubernetes-cluster-provisioning/</guid>
      <description> Provision RKE1 Cluster Management When RKE1 cluster show Provisioning Click Delete from menu  Expected Results  Can remove RKE1 Cluster and disapper on Cluster page RKE1 Cluster will be removed from rancher menu under explore cluster RKE1 virtual machine should be also be removed from Harvester  </description>
    </item>
    
    <item>
      <title>25-Delete RKE1 Kubernetes Cluster in Failure</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/25-delete-rke1-kubernetes-cluster-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/25-delete-rke1-kubernetes-cluster-failure/</guid>
      <description> Provision RKE1 Cluster Management When RKE1 cluster displayed in Failure Click Delete from menu  Expected Results  Can remove RKE1 Cluster and disapper on Cluster page RKE1 Cluster will be removed from rancher menu under explore cluster RKE1 virtual machine should be also be removed from Harvester  </description>
    </item>
    
    <item>
      <title>26-Delete RKE2 Kubernetes Cluster in Provisioning</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/26-delete-rke2-kubernetes-cluster-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/26-delete-rke2-kubernetes-cluster-provisioning/</guid>
      <description> Provision RKE2 Cluster Management When RKE2 cluster show Provisioning Click Delete from menu  Expected Results  Can remove RKE2 Cluster and disapper on Cluster page RKE2 Cluster will be removed from rancher menu under explore cluster RKE2 virtual machine should be also be removed from Harvester  </description>
    </item>
    
    <item>
      <title>27-Delete RKE2 Kubernetes Cluster in Failure</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/27-delete-rke2-kubernetes-cluster-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/27-delete-rke2-kubernetes-cluster-failure/</guid>
      <description> Provision RKE2 Cluster Management When RKE2 cluster displayed in Failure Click Delete from menu  Expected Results  Can remove RKE2 Cluster and disapper on Cluster page RKE2 Cluster will be removed from rancher menu under explore cluster RKE2 virtual machine should be also be removed from Harvester  </description>
    </item>
    
    <item>
      <title>28-Deploy Harvester cloud provider to RKE1 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/28-deploy-harvester-cloud-provider-to-rke1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/28-deploy-harvester-cloud-provider-to-rke1-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher  Environment Setup  Docker install rancher v2.6.3 Create one node harvester with enough resource  Verify steps  Environment preparation as above steps Import harvester to rancher from harvester settings Create cloud credential Create RKE1 node template  Provision a RKE1 cluster, check the Harvester as cloud provider  Access RKE1 cluster Open charts in Apps &amp;amp; Market page Install harvester cloud provider Make sure cloud provider installed complete  NAME: harvester-cloud-provider LAST DEPLOYED: Thu Dec 16 03:57:26 2021 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None --------------------------------------------------------------------- SUCCESS: helm install --namespace=kube-system --timeout=10m0s --values=/home/shell/helm/values-harvester-cloud-provider-100.</description>
    </item>
    
    <item>
      <title>29-Deploy Harvester cloud provider to RKE2 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/29-deploy-harvester-cloud-provider-to-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/29-deploy-harvester-cloud-provider-to-rke2-cluster/</guid>
      <description> Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Check alread set Harvester as cloud provider   Click Create Wait for RKE2 cluster provisioning complete (~20min)  Expected Results  Provision RKE2 cluster successfully with Running status   Can acccess RKE2 cluster to check all resources and services   Check cloud provider installed and configured on RKE2 cluster  </description>
    </item>
    
    <item>
      <title>30-Configure Harvester LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/30-configure-harvester-loadbalancer-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/30-configure-harvester-loadbalancer-service/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case
 Open Global Settings in hamburger menu Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Change ui-offline-preferred to Remote Refresh the current page (ctrl + r) Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Click Create Select Load Balancer   Given service name Provide Listending port and Target port   Click Add-on Config Provide Health Check port Select dhcp as IPAM mode Provide Health Check Threshold Provide Health Check Failure Threshold Provide Health Check Period Provide Health Check Timeout Click Create button  Expected Results  Can create load balance service correctly   Can operate and foward workload as expected  </description>
    </item>
    
    <item>
      <title>31-Specify &#34;pool&#34; IPAM mode in LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/31-specify-pool-ipam-mode-loadbalancer-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/31-specify-pool-ipam-mode-loadbalancer-service/</guid>
      <description>Prerequisite: Already provision RKE1/RKE2 cluster in previous test case
 Open Global Settings in hamburger menu Replace ui-dashboard-index to https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Change ui-offline-preferred to Remote Refresh the current page (ctrl + r) Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Click Create Select Load Balancer   Given service name Provide Listending port and Target port   Click Add-on Config Provide Health Check port Select pool as IPAM mode Provide Health Check Threshold Provide Health Check Failure Threshold Provide Health Check Period Provide Health Check Timeout Click Create button  Expected Results  Can create load balance service correctly Can operate and foward workload as expected  </description>
    </item>
    
    <item>
      <title>32-Deploy Harvester CSI provider to RKE 1 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/32-deploy-harvester-csi-provider-to-rke1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/32-deploy-harvester-csi-provider-to-rke1-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher  Environment Setup  Docker install rancher v2.6.3 Create one node harvester with enough resource  Verify steps  Environment preparation as above steps Import harvester to rancher from harvester settings Create cloud credential Create RKE1 node template  Provision a RKE1 cluster, check the Harvester as cloud provider  Access RKE1 cluster Open charts in Apps &amp;amp; Market page Install Harvester CSI driver Make sure CSI driver installed complete  NAME: harvester-csi-driver LAST DEPLOYED: Thu Dec 16 03:59:54 2021 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Successfully deployed Harvester CSI driver to the kube-system namespace.</description>
    </item>
    
    <item>
      <title>33-Deploy Harvester CSI provider to RKE 2 Cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/33-deploy-harvester-csi-provider-to-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/33-deploy-harvester-csi-provider-to-rke2-cluster/</guid>
      <description> Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Check alread set Harvester as cloud provider   Click Create Wait for RKE2 cluster provisioning complete (~20min)  Expected Results  Provision RKE2 cluster successfully with Running status   Can acccess RKE2 cluster to check all resources and services   Check CSI driver installed and configured on RKE2 cluster   </description>
    </item>
    
    <item>
      <title>34-Hot plug and unplug volumes in RKE1 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/34-hotplug-unplug-volumes-in-rke1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/34-hotplug-unplug-volumes-in-rke1-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher  Environment Setup  Docker install rancher v2.6.3 Create one node harvester with enough resource  Verify Steps   Environment preparation as above steps
  Import harvester to rancher from harvester settings
  Create cloud credential
  Create RKE1 node template   Provision a RKE1 cluster, check the Harvester as cloud provider   Access RKE1 cluster</description>
    </item>
    
    <item>
      <title>35-Hot plug and unplug volumes in RKE2 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/35-hotplug-unplug-volumes-in-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/35-hotplug-unplug-volumes-in-rke2-cluster/</guid>
      <description>Related task: #1396 Integration Cloud Provider for RKE1 with Rancher  Environment Setup  Docker install rancher v2.6.3 Create one node harvester with enough resource  Verify Steps   Environment preparation as above steps
  Import harvester to rancher from harvester settings
  Create cloud credential
  Create RKE2 cluster as test case #34
  Access RKE2 cluster
  Open charts in Apps &amp;amp; Market page</description>
    </item>
    
    <item>
      <title>36-Remove Harvester LoadBalancer service</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/36-remove-harvester-loadbalancer-service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/36-remove-harvester-loadbalancer-service/</guid>
      <description> Open provisioned RKE2 cluster from hamburger menu Drop down Service Discovery Click Services Delete previous created load balancer service  Expected Results  Can remove load balance service correctly Service will be removed from assigned Apps  </description>
    </item>
    
    <item>
      <title>37-Import Online Harvester From the Airgapped Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher-copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher-copy/</guid>
      <description>Environment Setup Setup the online harvester
 Use ipxe vagrant example to setup a 3 nodes cluster https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create ubuntu cloud image from URL Create virtual machine with name vlan1 and id: 1 Create virtual machine and assign vlan network, confirm can get ip address  Setup squid HTTP proxy server
 Move to vagrant pxe harvester folder Execute vagrant ssh pxe_server Run apt-get install squid Edit /etc/squid/squid.</description>
    </item>
    
    <item>
      <title>37-Import Online Harvester From the Airgapped Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/37-import-online-harvester-from-airgapped-rancher/</guid>
      <description>Environment Setup Setup the online harvester
 Use ipxe vagrant example to setup a 3 nodes cluster https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create ubuntu cloud image from URL Create virtual machine with name vlan1 and id: 1 Create virtual machine and assign vlan network, confirm can get ip address  Setup squid HTTP proxy server
 Move to vagrant pxe harvester folder Execute vagrant ssh pxe_server Run apt-get install squid Edit /etc/squid/squid.</description>
    </item>
    
    <item>
      <title>38-Import Airgapped Harvester From the Airgapped Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/38-import-airgapped-harvester-from-airgapped-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/38-import-airgapped-harvester-from-airgapped-rancher/</guid>
      <description>Related task: #1052 Test Air gap with Rancher integration  Environment Setup Setup the airgapped harvester
 Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create virtual machine with name vlan1 and id: 1 Open Settings, edit http-proxy with the following values  HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    
    <item>
      <title>39-Standard user no Harvester Access</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/39-rbac-standard-user-no-access/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/39-rbac-standard-user-no-access/</guid>
      <description> As admin import/register a harvester cluster in Rancher As admin, Enable Harvester node driver As a standard user User1, login to rancher Verify User1 has no access to harvester cluster in Virtualization management page Create harvester cloud credential as User1 Verify User1 can use this cloud credential to create a node template and a node driver cluster 3 and can CRUD each resource  </description>
    </item>
    
    <item>
      <title>40-RBAC Add restricted admin User Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/40-rbac-add-restricted-admin-user-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/40-rbac-add-restricted-admin-user-harvester/</guid>
      <description> As admin import/register a harvester cluster in Rancher create restricted admin user rstradm verify rstradm has access to to Virturalization management page and the harvester cluster is listed Verify rstradm has access to Harvester UI through rancher by selecting it from the list in step 3 and can CRUD each resource  </description>
    </item>
    
    <item>
      <title>41-Import Harvester into nested Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/41-rancher-nested-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/41-rancher-nested-harvester/</guid>
      <description>Prerequisite: External network on VLAN
 Install Rancher in a VM using Docker method on Harvester cluster using the external VLAN Login rancher dashboard Navigate to Virtual Management Page Click import existing Copy the curl command  SSH to harvester master node (user: rancher) Execute the curl command to import harvester to rancher curl --insecure -sfL https://192.168.50.82/v3/import/{identifier}.yaml | kubectl apply -f - Run sudo chmod 775 /etc/rancher/rke2/rke2.yaml to solve the permission denied error Run curl command again, you should see the following successful import message namespace/cattle-system configured serviceaccount/cattle created clusterrolebinding.</description>
    </item>
    
    <item>
      <title>42-Add cloud credential KUBECONFIG</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/42-add-cloud-credential-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/42-add-cloud-credential-kubeconfig/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester
 Click Cluster Management Click Cloud Credentials Click createa and select Harvester Input credential name Select external cluster Input KUBECONFIG from Harvester Click Create  </description>
    </item>
    
    <item>
      <title>43-Scale up node driver RKE1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/43-node-driver-scale-up-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/43-node-driver-scale-up-rke1/</guid>
      <description>Prerequisite: RKE1 cluster in Harvester with at least 2 worker nodes
 provision a multinode cluster using harvester node driver with at least 2 worker nodes scale up a node in the cluster  </description>
    </item>
    
    <item>
      <title>44-Scale up node driver RKE2</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/44-node-driver-scale-up-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/44-node-driver-scale-up-rke2/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester
 provision a multinode cluster using harvester node driver with at least 2 worker nodes scale up a node in the cluster  </description>
    </item>
    
    <item>
      <title>45-Scale down node driver RKE1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/45-node-driver-scale-down-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/45-node-driver-scale-down-rke1/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester
 provision a multinode cluster using harvester node driver with at least 2 worker nodes scale down a node in the cluster  </description>
    </item>
    
    <item>
      <title>46-Scale down node driver RKE2</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/46-node-driver-scale-down-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/46-node-driver-scale-down-rke2/</guid>
      <description>Prerequisite: KUBECONFIG from Harvester
 provision a multinode cluster using harvester node driver with at least 2 worker nodes scale down a node in the cluster  </description>
    </item>
    
    <item>
      <title>47-Verify Backup and restore on same server</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/47-verify-backup-restore-same-server/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/47-verify-backup-restore-same-server/</guid>
      <description></description>
    </item>
    
    <item>
      <title>48-Verify Backup and restore on server migration</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/48-verify-backup-restore-server-migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/48-verify-backup-restore-server-migration/</guid>
      <description> Create a harvester cluster Deploy a harvester node driver cluster preupgrade checks on both Take a backup Restore on a new rancher server from backup taken ins step 4 Run post-upgrade checks for both clusters Verify virtualization management → harvester is accessible Run p0 use case  </description>
    </item>
    
    <item>
      <title>49-Overprovision Harvester</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/49-overprovision-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/49-overprovision-harvester/</guid>
      <description> import harvester into rancher over-provision the connected harvester cluster (i.e. deploy large number of nodes) note: the number will depend on the resources available in the harvester cluster you&amp;rsquo;ve imported. i.e. a harvester setup with 24 cores, 64 GB of ram, you could try provisioning a 3cp, 2cp, 2w cluster of size 4 vCPU 8GB ram to over-provision CPU i.e. a harvester setup with 24 cores, 64 GB of ram, you could try provisioning a 3cp, 2cp, 2w cluster of size 2 vCPU 10GB ram to over-provision CPU  </description>
    </item>
    
    <item>
      <title>50-Use fleet when a harvester cluster is imported to rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/50-fleet-with-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/50-fleet-with-harvester/</guid>
      <description> deploy rancher with harvester enabled docker: &amp;ndash;features=harvester=enabled helm: &amp;ndash;set &amp;lsquo;extraEnv[0].name=CATTLE_FEATURES&amp;rsquo; &amp;ndash;set &amp;lsquo;extraEnv[0].value=harvester=enabled import a harvester setup go to fleet → repos -&amp;gt; create validate that that the harvester cluster is NOT in the dropdown for cluster deployments validate that selecting the &amp;lsquo;all clusters&amp;rsquo; option for deployment does NOT deploy to the harvester cluster  </description>
    </item>
    
    <item>
      <title>51-Use harvester cloud provider to provision an LB - rke1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/51-harvester-cloud-provider-loadbalancer-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/51-harvester-cloud-provider-loadbalancer-rke1/</guid>
      <description> Provision cluster using rke1 with harvester as the node driver enable the cloud driver for harvester while provisioning the cluster run jenkins v3 validation checks once cluster comes to active  </description>
    </item>
    
    <item>
      <title>52-Use harvester cloud provider to provision an LB - rke2</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/52-harvester-cloud-provider-loadbalancer-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/52-harvester-cloud-provider-loadbalancer-rke2/</guid>
      <description> Provision cluster using rke1 with harvester as the node driver enable the cloud driver for harvester while provisioning the cluster run jenkins v3 validation checks once cluster comes to active  </description>
    </item>
    
    <item>
      <title>53-Disable Harvester flag with Harvester cluster added</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/53-disable-harvester-flag/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/53-disable-harvester-flag/</guid>
      <description>Pre-requisites: Rancher with Harvester imported
 Disable Harvester feature flag on Rancher  Expected Results  Harvester should show up in cluster management Virtualization management tab should be hidden.  </description>
    </item>
    
    <item>
      <title>54-Import Airgapped Harvester From the Online Rancher</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/54-import-airgapped-harvester-from-online-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/54-import-airgapped-harvester-from-online-rancher/</guid>
      <description>Environment Setup Setup the airgapped harvester
 Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Open Settings, edit http-proxy with the following values  HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.0.0/16,cattle-system.svc,.svc,.cluster.local,&amp;lt;internal domain&amp;gt;  Create ubuntu cloud image from URL Create virtual machine with name vlan1 and id: 1 Create virtual machine and assign vlan network, confirm can get ip address  Setup squid HTTP proxy server</description>
    </item>
    
    <item>
      <title>55-Import Harvester to Rancher in airgapped different subnet</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/55-import-harvester-rancher-airgapped-different-subnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/55-import-harvester-rancher-airgapped-different-subnet/</guid>
      <description>Environment Setup Note: Harvester and Rancher are under different subnet, can access to each other
Setup the airgapped harvester
 Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Enable vlan on harvester-mgmt Create virtual machine with name vlan1 and id: 1 Open Settings, edit http-proxy with the following values  HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    
    <item>
      <title>56-Import Harvester to Rancher in airgapped different subnet</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/56-import-harvester-rancher-online-different-subnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/56-import-harvester-rancher-online-different-subnet/</guid>
      <description>Environment Setup Note: Harvester and Rancher are under different subnet, can access to each other
Setup the online harvester
 Iso or vagrant ipxe install harvester on network with internet connection Enable vlan on harvester-mgmt Create virtual machine with name vlan1 and id: 1 Create ubuntu cloud image from URL Create virtual machine and assign vlan network, confirm can get ip address  Setup the online rancher
 Install rancher on network with internet connection throug docker command  $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.</description>
    </item>
    
    <item>
      <title>57-Import airgapped harvester from airgapped rancher rke1</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/57-import-airgapped-harvester-from-airgapped-rancher-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/57-import-airgapped-harvester-from-airgapped-rancher-rke1/</guid>
      <description>Related task: #1052 Test Air gap with Rancher integration  Environment Setup Setup the airgapped harvester
 Fetch ipxe vagrant example with new offline feature https://github.com/harvester/ipxe-examples/pull/32 Edit the setting.xml file Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster Enable vlan on harvester-mgmt Now harvester dashboard page will out of work Create virtual machine with name vlan1 and id: 1 Open Settings, edit http-proxy with the following values  HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    
    <item>
      <title>58-Negative-Fully power cycle harvester node machine should recover RKE2 cluster</title>
      <link>https://harvester.github.io/tests/manual/harvester-rancher/58-negative-fully-power-cycle-harvester-node-machine-should-recover-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/harvester-rancher/58-negative-fully-power-cycle-harvester-node-machine-should-recover-rke2-cluster/</guid>
      <description>Related issue: #1561 Fully shutdown then power on harvester node machine can&amp;rsquo;t get provisioned RKE2 cluster back to work
  Related issue: #1428 rke2-coredns-rke2-coredns-autoscaler timeout
  Environment Setup  The network environment must have vlan network configured and also have DHCP server prepared on your testing vlan  Verification Step  Prepare a 3 nodes harvester cluster (provo bare machine) Enable virtual network with harvester-mgmt Create vlan1 with id 1 Import harvester from rancher and create cloud credential Provision a RKE2 cluster with vlan 1 Wait for build up ready Shutdown harvester node 3 Shutdown harvester node 2 Shutdown harvester node 1 Wait for 20 minutes Power on node 1, wait 10 seconds Power on node 2, wait 10 seconds Power on node 3 Wait for harvester startup complete Wait for RKE2 cluster back to work Check node and VIP accessibility Check the rke2-coredns pod status kubectl get pods --all-namespaces | grep rke2-coredns  Expected Results   RKE2 cluster on harvester can recover to Active status</description>
    </item>
    
    <item>
      <title>Add a custom &#34;Docker Install URL&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-docker-install-url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-docker-install-url/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Add a custom &#34;Insecure Registries&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-insecure-registries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-insecure-registries/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Go to node, execute docker info, check the &amp;ldquo;Insecure Registries&amp;rdquo; setting is &amp;ldquo;harbor.wujing.site&amp;rdquo;  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Add a custom &#34;Registry Mirrors&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-registry-mirrors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-registry-mirrors/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Go to node, execute &amp;ldquo;docker info&amp;rdquo;, check the &amp;ldquo;Registry Mirrors&amp;rdquo; setting is &amp;ldquo;https://s06nkgus.mirror.aliyuncs.com&amp;rdquo;  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Add a custom &#34;Storage Driver&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-custom-storage-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-custom-storage-driver/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration Go to node, execute &amp;ldquo;docker info&amp;rdquo;, check the Storage Driver setting is overlay  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Add a network to an existing VM with only 1 network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-only-1-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-only-1-network/</guid>
      <description> Add a network to the VM Save the VM Wait for it to start/restart  Expected Results  the VM should start successfully The already existing network connectivity should still work The new connectivity should also work  </description>
    </item>
    
    <item>
      <title>Add a network to an existing VM with two networks</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-two-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/add-a-network-to-an-existing-vm-with-two-networks/</guid>
      <description> Add a network to the VM Save the VM Wait for it to start/restart  Expected Results  the VM should start successfully The already existing network connectivity should still work The new connectivity should also work  </description>
    </item>
    
    <item>
      <title>Add a node to existing cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/add-node-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/add-node-cluster/</guid>
      <description> Start with harvester installer and select &amp;lsquo;Join an existing Harvester cluster&amp;rsquo; Provide the management ip and cluster token  Expected Results  On completion, Harvester should show the same management url as of existing node and status as ready. Check the host section, the joined node must appear  </description>
    </item>
    
    <item>
      <title>Add cluster driver</title>
      <link>https://harvester.github.io/tests/manual/node-driver/add-cluster-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/add-cluster-driver/</guid>
      <description> Cluster Management &amp;gt; Drivers &amp;gt; Node Drivers Click &amp;ldquo;Add Node driver&amp;rdquo; Add the correct configuration and save  Expected Results  Created successfully, status is active  </description>
    </item>
    
    <item>
      <title>Add Labels (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/add-labels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/add-labels/</guid>
      <description> Add multiple labels to the images. Click save  Expected Results  Labels should be added successfully  </description>
    </item>
    
    <item>
      <title>Add multiple Networks via form</title>
      <link>https://harvester.github.io/tests/manual/network/add-multiple-networks-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/add-multiple-networks-form/</guid>
      <description> Create a new VM via the web form Add both a management network and an external VLAN network Validate both interfaces exist in the VM  ip link list   Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine  Expected Results  The VM should create You should see three interfaces listed in VM You should get responses from pinging the VM You should get responses from pinging the VM  </description>
    </item>
    
    <item>
      <title>Add multiple Networks via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/add-multiple-networks-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/add-multiple-networks-yaml/</guid>
      <description> Create a new VM via YAML Add both a management network and an external VLAN network Validate both interfaces exist in the VM  ip link list   Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine  Expected Results  The VM should create You should see three interfaces listed in VM You should get responses from pinging the VM You should get responses from pinging the VM  </description>
    </item>
    
    <item>
      <title>Add network reachability detection from host for the VLAN network</title>
      <link>https://harvester.github.io/tests/manual/_incoming/add-network-reachability-detection-from-host-for-vlan-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/add-network-reachability-detection-from-host-for-vlan-network/</guid>
      <description>Related issue: #1476 Add network reachability detection from host for the VLAN network  Category:  Network  Environment Setup  The network environment must have vlan network configured and also have DHCP server prepared on your testing vlan  Verification Steps  Enable virtual network with harvester-mgmt in harvester Create VLAN 806 with id 806 and set to default auto mode Import harvester to rancher 1 .Create cloud credential Provision a rke2 cluster to harvester    Deploy a nginx server workload     Open Service Discover -&amp;gt; Services</description>
    </item>
    
    <item>
      <title>Add the different roles to the cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/q-cluster-different-roles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/q-cluster-different-roles/</guid>
      <description> Create three users user1, user2, user3 Give the roles of Cluster Owner to user1, Create Project to user2 and Cluster Member to user3 respectively. Login with these three roles  Expected Results </description>
    </item>
    
    <item>
      <title>Add VLAN network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/add-vlan-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/add-vlan-network/</guid>
      <description>Environment setup This should be done on a Harvester setup with at least 2 NICs and at least 2 nodes. This is easily tested in Vagrant
Verification Steps  Open settings on a harvester cluster Navigate to the VLAN settings page Click Enabled Check dropdown for NICs and verify that percentage is showing 100% Add the NIC Click Save Validate that it has updated in settings  Expected Results  You should be able to add the VLAN network device You should see in the settings list that it has your new default NIC  </description>
    </item>
    
    <item>
      <title>Add/remove a node in the created harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-add-remove-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-add-remove-node/</guid>
      <description> add/remove a node in the created harvester cluster  Expected Results  rancher on the cluster modified successfully harvester corresponding VM node added/removed successfully  </description>
    </item>
    
    <item>
      <title>Add/remove disk to Host config</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1623-add-disk-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1623-add-disk-to-host/</guid>
      <description> Related issues: #1623 Unable to add additional disks to host config  Environment setup  Add Disk that isn&amp;rsquo;t assigned to host  Verification Steps  Head to &amp;ldquo;Hosts&amp;rdquo; page Click &amp;ldquo;Edit Config&amp;rdquo; on a node and switch to &amp;ldquo;Disks&amp;rdquo; tab Validate: Open dropdown and see no disks Attach a disk on that node Validate: Open dropdown and see some disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space Detach a disk on that node Validate: Open dropdown and see no disks Verify that host shows new disk as available storage and Longhorn is showing new schedulable space  Expected Results  Disk space should show appropriately   </description>
    </item>
    
    <item>
      <title>Additional trusted CA configure-ability</title>
      <link>https://harvester.github.io/tests/manual/_incoming/additional-trusted-ca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/additional-trusted-ca/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1260
Verify Items  Image download with self-signed additional-ca VM backup with self-signed additional-ca  Case: Image downlaod  Install Harvester with ipxe-example which includes https://github.com/harvester/ipxe-examples/pull/36 Upload any valid iso to pxe-server&amp;rsquo;s /var/www/ Use Browser to access https://&amp;lt;pxe-server-ip&amp;gt;/&amp;lt;iso-file&amp;gt; should be valid Add self-signed cert to Harvester  Navigate to Harvester Advanced Settings, edit additional-ca cert content can be retrieved in pxe-server /etc/ssl/certs/nginx-selfsigned.crt   Create Image with the same URL https://&amp;lt;pxe-server-ip&amp;gt;/&amp;lt;iso-file&amp;gt; Image should be downloaded  Case: VM backup  Install Harvester with ipxe-example setup Minio in pxe-server  follow instruction to download binary and start the service login to UI console then add region and create bucket follow instruction to generate self-signed cert with IP SANs restart service with self-signed cert   Add self-signed cert to Harvester Add local Minio info as S3 into backup-target Backup-Target Should not pop up any Error Message Create Image for VM creation Create VM with any resource Perform VM backup VM&amp;rsquo;s data Should be backup into Minio&amp;rsquo;s folder  </description>
    </item>
    
    <item>
      <title>Agent Node should not rely on specific master Node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/agent_node_connectivity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/agent_node_connectivity/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1521
Verify Items  Agent Node should keep connection when any master Node is down  Case: Agent Node&amp;rsquo;s connecting status  Install Harvester with 4 nodes which joining node MUST join by VIP (point server-url to use VIP) Make sure all nodes are ready  Login to dashboard, check host state become Active SSH to the 1st node, run command kubectl get node to check all STATUS should be Ready   SSH to agent nodes which ROLES IS &amp;lt;none&amp;gt; in Step 2i&amp;rsquo;s output  Output should contains VIP in the server URL, by run command cat /etc/rancher/rke2/config.</description>
    </item>
    
    <item>
      <title>allow users to create cloud-config template on the VM creating page</title>
      <link>https://harvester.github.io/tests/manual/_incoming/allow-users-to-create-cloud-config-template-on-vm-creating-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/allow-users-to-create-cloud-config-template-on-vm-creating-page/</guid>
      <description> Related issues: #1433 allow users to create cloud-config template on the VM creating page  Category:  Virtual Machine  Verification Steps  Create a new virtual machine Click advanced options Drop down user data template -&amp;gt; create new Drop down network data template -&amp;gt; create new  Expected Results  User can create user and network data template when create virtual machine Created cloud-init template template can be saved and auto selected to the latest one   </description>
    </item>
    
    <item>
      <title>Attach unpartitioned NVMe disks to host</title>
      <link>https://harvester.github.io/tests/manual/_incoming/attach-unpartitioned-nvme-disks-to-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/attach-unpartitioned-nvme-disks-to-host/</guid>
      <description>Related issues: #1414 Adding unpartitioned NVMe disks fails  Category:  Storage  Verification Steps  Use qemu-img create -f qcow2 command to create three disk image locally Shutdown target node VM machine Directly edit VM xml content in virt manager page Add to the first line Add the following line before the end of quote  &amp;lt;qemu:commandline&amp;gt; &amp;lt;qemu:arg value=&amp;quot;-drive&amp;quot;/&amp;gt; &amp;lt;qemu:arg value=&amp;quot;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme301.img,if=none,id=D22&amp;quot;/&amp;gt; &amp;lt;qemu:arg value=&amp;quot;-device&amp;quot;/&amp;gt; &amp;lt;qemu:arg value=&amp;quot;nvme,drive=D22,serial=1234&amp;quot;/&amp;gt; &amp;lt;qemu:arg value=&amp;quot;-drive&amp;quot;/&amp;gt; &amp;lt;qemu:arg value=&amp;quot;file=/home/davidtclin/Documents/Software/qemu_kvm/node_3/nvme302.</description>
    </item>
    
    <item>
      <title>Authentication Validation</title>
      <link>https://harvester.github.io/tests/manual/authentication/general-authentication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/general-authentication/</guid>
      <description>Enable Access Control . Choose “Allow any valid User” as “Site Access”. Make sure any user is able to access the site. Enable Access Control . Choose “Restrict to Specific User” and add few users. Make sure only the specified users have access to the server. Others should get authentication error. Enable Access Control . Choose “Restrict to Specific User” and add a group. Make sure only all users belonging to the group have access to the server Others should get authentication error.</description>
    </item>
    
    <item>
      <title>Automatically get VIP during PXE installation</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1410-pxe-installation-automatically-get-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1410-pxe-installation-automatically-get-vip/</guid>
      <description> Related issues: #1410 Support getting VIP automatically during PXE boot installation  Verification Steps  Comment vip and vip_hw_addr in ipxe-examples/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2 Start vagrant-pxe-harvester Run kubectl get cm -n harvester-system vip  Check whether we can get ip and hwAddress in it   Run ip a show harvester-mgmt  Check whether there are two IPs in it and one is the vip.    Expected Results  VIP should automatically be assigned  </description>
    </item>
    
    <item>
      <title>Backup and restore of harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/q-cluster-backup-restore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/q-cluster-backup-restore/</guid>
      <description> create a deployment in harvester cluster Go to the rancher&amp;rsquo;s cluster list and make a backup of the harvester cluster After the backup is complete, delete the deployment created in the harvester cluster go to the list of clusters in the rancher and restore the harvester cluster  Expected Results </description>
    </item>
    
    <item>
      <title>Backup S3 reduce permissions</title>
      <link>https://harvester.github.io/tests/manual/_incoming/backup_s3_permission/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/backup_s3_permission/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1339
Verify Items  Backup target connect to S3 should only require the permission to access the specific bucket  Case: S3 Backup with single-bucket-user  Install Harvester with any nodes Setup Minio  then follow the instruction to create a single-bucket-user. Create specific bucket for the user Create other buckets   setup backup-target with the single-bucket-user permission  When assign the dedicated bucket (for the user), connection should success.</description>
    </item>
    
    <item>
      <title>Backup Single VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm/</guid>
      <description> Click take backup in virtual machine list  Expected Results  Backup should be created Backup should be listed in backups list Backup should be available on remote storage (S3/NFS)  </description>
    </item>
    
    <item>
      <title>Backup Single VM that has been live migrated before (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-that-has-been-live-migrated/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-that-has-been-live-migrated/</guid>
      <description> Click take backup in virtual machine list  Expected Results  Backup should be created Backup should be listed in backups list Backup should be available on remote storage (S3/NFS)  </description>
    </item>
    
    <item>
      <title>Backup single VM with node off</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-node-off/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/backup-single-vm-node-off/</guid>
      <description>On multi-node setup bring down node that is hosting VM Click take backup in virtual machine list  Expected Results  The backup should complete successfully  Comments We do allow taking backup even if the VM is down, as you can take backup when the VM is off, this is because the volume still exists with longhorn&amp;rsquo;s multi replicas, but weneed to check the data integrity.
Known Bugs https://github.</description>
    </item>
    
    <item>
      <title>Backup Target error message</title>
      <link>https://harvester.github.io/tests/manual/_incoming/backup_target_errmsg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/backup_target_errmsg/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1051
Verify Items  Backup target should check input before Click Save Error message should displayed on edit page when input is wrong  Case: Connect to invalid Backup Target  Install Harvester with any node Login to dashboard, then navigate to Advanced Settings Edit backup-target,then input invalid data for NFS/S3 and click Save The Page should not be redirect to Advanced Settings Error Message should displayed under Save button  </description>
    </item>
    
    <item>
      <title>Basic functional verification of Harvester cluster after creation</title>
      <link>https://harvester.github.io/tests/manual/node-driver/verify-cluster-functionality/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/verify-cluster-functionality/</guid>
      <description> create the project. deploy deployment  Expected Results  The project is created successfully Deployment successfully deployed  </description>
    </item>
    
    <item>
      <title>Better Load Balancer Config of Harvester cloud provider</title>
      <link>https://harvester.github.io/tests/manual/_incoming/better-load-balancer-config-rke2-cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/better-load-balancer-config-rke2-cloud-provider/</guid>
      <description>Related issue: #1435 better loadblancer config of Harvester cloud provider  Category:  Rancher Integration  Environment setup  Install rancher 2.6.3 by docker  docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps  Import harvester to rancher virtualization management Create a harvester cluster by harvester driver Access the new harvester cluster from rancher cluster management Create a load balancer from service discovery -&amp;gt; services Re login rancher Open create load-balance page Click ctrl+R to refresh page Check the &amp;ldquo;Add-on Config&amp;rdquo; tabs  Expected Results   User can configure port, IPAM and health check related setting on Add-on Config page   Can create load balancer correctly with health check setting</description>
    </item>
    
    <item>
      <title>Button of `Download KubeConfig`</title>
      <link>https://harvester.github.io/tests/manual/_incoming/download_kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/download_kubeconfig/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1349
Verify Items  Download KubeConfig should not exist in general views Download Kubeconfig should exist in Support page Downloaded file should be named with suffix .yaml  Case: Download KubeConfig  navigate to every pages to make sure download kubeconfig icon will not appear in header section navigate to support page to check Download KubeConfig is work normally  </description>
    </item>
    
    <item>
      <title>Chain VM templates and images</title>
      <link>https://harvester.github.io/tests/manual/_incoming/760-chained-vm-templates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/760-chained-vm-templates/</guid>
      <description> Related issues: #760 cloud config byte limit  Verification Steps  Create a vm and add userData or networkData, test if it works Run VM health checks create a vm template and add userData create a new vm and use the template Run VM health checks use the existing vm to generate a template, then use the template to create a new vm Run VM health Checks  Expected Results  All VM&amp;rsquo;s should create All VM Health Checks should pass  </description>
    </item>
    
    <item>
      <title>Change api-ui-source bundled</title>
      <link>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-bundled/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-bundled/</guid>
      <description> Log in as admin Navigate to advanced settings Change api-ui-source to bundled Save Refresh page Check page source for dashboard loading location  Expected Results  Log in should complete Settings should save dashboard location should be loading from /dashboard/_nuxt/  (verify it in browser&amp;rsquo;s developers tools)    </description>
    </item>
    
    <item>
      <title>Change api-ui-source external</title>
      <link>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-external/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/chage-api-ui-source-external/</guid>
      <description> Log in as admin Navigate to advanced settings Change api-ui-source to external Save Refresh page Check page source for dashboard loading location  Expected Results  Log in should complete Settings should save dashboard location should be loading from https://releases.rancher.com/harvester-ui/latest (verify it in browser&amp;rsquo;s developers tools)  </description>
    </item>
    
    <item>
      <title>Change DNS servers while installing</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1590-change-dns-server-for-install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1590-change-dns-server-for-install/</guid>
      <description>Related issues: #1590 Harvester installer can&amp;rsquo;t resolve hostnames  Known Issues When supplying multiple ip=&amp;hellip; kernel cmdline arguments, only one of them will be configured by dracut, therefore only the configured interface would have ifcfg generated. So for now, we can&amp;rsquo;t support multiple ip=&amp;hellip; kernel cmdline arguments
Verification Steps   Because configuring the network of the installation environment only works with PXE installation, you could use ipxe-examples/vagrant-pxe-harvester/ to set it up.</description>
    </item>
    
    <item>
      <title>Change log level debug</title>
      <link>https://harvester.github.io/tests/manual/advanced/change-log-level-debug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/change-log-level-debug/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on log-level Choose Debug Save Create two VMs Reboot both VMs Download Logs  Expected Results  Login should complete Settings should save VMs should create VMs should reboot sucessfully Logs should show Debug level output  </description>
    </item>
    
    <item>
      <title>Change log level Info</title>
      <link>https://harvester.github.io/tests/manual/advanced/change-log-level-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/change-log-level-info/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on log-level Choose Info Save Create two VMs Reboot both VMs Download Logs  Expected Results  Login should complete Settings should save VMs should create VMs should reboot sucessfully Logs should show Info level output  </description>
    </item>
    
    <item>
      <title>Change log level Trace</title>
      <link>https://harvester.github.io/tests/manual/advanced/change-log-level-trace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/change-log-level-trace/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on log-level Choose Trace Save Create two VMs Reboot both VMs Download Logs  Expected Results  Login should complete Settings should save VMs should create VMs should reboot sucessfully Logs should show Trace level output  </description>
    </item>
    
    <item>
      <title>Change user passowrd</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1409-change-password/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1409-change-password/</guid>
      <description> Related issues: #1409 There&amp;rsquo;s no way to change user password in single cluster UI  Verification Steps  Logged in with user Changed password Logged out Logged back in with new password Verified old password didn&amp;rsquo;t work  Expected Results  Password should change and be accepted on new login Old password shouldn&amp;rsquo;t work  </description>
    </item>
    
    <item>
      <title>Check can apply the resource quota limit to project and namespace</title>
      <link>https://harvester.github.io/tests/manual/_incoming/check-can-apply-the-resource-quota-limit-to-project-and-namespace-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/check-can-apply-the-resource-quota-limit-to-project-and-namespace-/</guid>
      <description>Related issues: #1454 Incorrect memory unit conversion in namespace resource quota  Category:  Rancher Integration  Environment setup  Install the latest rancher from docker command  $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps  Access Rancher dashboard Open Cluster management -&amp;gt; Explore the active cluster Create a new project test-1454-proj in Projects/Namespaces Set resource quota for the project   Memory Limit:  Project Limit: 512 Namespace default limit: 256   Memory Reservation:  Project Limit: 256 Namespace default limit: 128     Click create namespace test-1454-ns under project test-1454-proj Click Kubectl Shell and run the following command   kubectl get ns kubectl get quota -n test-1454-ns   Check the output Click Workload -&amp;gt; Deployments -&amp;gt; Create Given the Name, Namespace and Container image  Click Create  Expected Results Based on configured project resource limit and namespace default limit,</description>
    </item>
    
    <item>
      <title>Check crash dump when there&#39;s a kernel panic</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1357-kernel-panic-check-crash-dump/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1357-kernel-panic-check-crash-dump/</guid>
      <description> Related issues: #1357 Crash dump not written when kernel panic occurs  Verification Steps  Created new single node cluster with 16GB RAM Booted into debug mode from GRUB entry Created several VMs triggered kernel panic with echo c &amp;gt;/proc/sysrq-trigger Waited for reboot Verified that dump was saved in /var/crash  Expected Results  dump should be saved in /var/crash  </description>
    </item>
    
    <item>
      <title>Check default and customized project and namespace details page</title>
      <link>https://harvester.github.io/tests/manual/_incoming/check-default-customized-project-and-namespace-details-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/check-default-customized-project-and-namespace-details-page/</guid>
      <description> Related issue: #1574 Multi-cluster projectNamespace details page error  Category:  Rancher Integration  Environment setup  Install rancher 2.6.3 by docker  docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps  Import harvester from rancher dashboard Access harvester from virtualization management page Create several new projects Create several new namespaces under each new projects Access all default and self created namespace Check can display namespace details Check all new namespaces can display correctly under each projects  Expected Results  Access harvester from rancher virtualization management page Click any namespace in the Projects/Namespace can display details correctly with no page error  Default namespace Customized namespace  Newly created namespace will display under project list   </description>
    </item>
    
    <item>
      <title>check detailed network status in host page</title>
      <link>https://harvester.github.io/tests/manual/_incoming/check-detailed-network-status-in-host-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/check-detailed-network-status-in-host-page/</guid>
      <description>Related issues: #531 Better error messages when misconfiguring multiple nics  Category:  Host  Verification Steps  Enable vlan cluster network setting and set a default network interface Wait a while for the setting take effect on all harvester nodes Click nodes on host page Check the network tab  Expected Results On the Host view page, now we can see detailed network status including Name, Type, IP Address, Status etc.</description>
    </item>
    
    <item>
      <title>Check favicon and title on pages</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1520-check-title-and-favicon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1520-check-title-and-favicon/</guid>
      <description> Related issues: #1520 incorrect title and favicon  Verification Steps  Log into Harvester Check page title and favicon on each of these pages  dashboard main page settings support Volumes SSH Keys Host info    Expected Results  Harvester favicon and title should show on each page  </description>
    </item>
    
    <item>
      <title>Check Longhorn volume mount point</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1667-check-longhorn-volume-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1667-check-longhorn-volume-mount/</guid>
      <description> Related issues: #1667 data partition is not mounted to the LH path properly  Verification Steps  Install Harvester node in VM from ISO Check partitions with lsblk -f Verify mount point of /var/lib/longhorn  Expected Results  Mount point should show /var/lib/longhorn   </description>
    </item>
    
    <item>
      <title>Check redirect for editing server URL setting</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1489-redirect-for-server-url-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1489-redirect-for-server-url-setting/</guid>
      <description> Related issues: #1489 Edit Advanced Setting option server-url will redirect to inappropriate page  Verification Steps  Install harvester Access harvester Edit server-url form settings Check server-url save, cancel, and back. Additional context:   Expected Results  URL should stay the same when navigating  </description>
    </item>
    
    <item>
      <title>Check Terms and Conditions Link</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1634-terms-and-conditions-link/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1634-terms-and-conditions-link/</guid>
      <description> Related issues: #1634 Welcome screen asks to agree to T&amp;amp;Cs for using Rancher not Harvester  Verification Steps  Install Harvester Go to management page and see last line (before Continue button) Verify link to SUSE EULA https://www.suse.com/licensing/eula/  Expected Results  Link should go to SUSE EULA   </description>
    </item>
    
    <item>
      <title>Check that you can communicate with the Harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/terraformer/harvester-cluster-communicate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/harvester-cluster-communicate/</guid>
      <description>Set the KUBECONFIG env variable with the path of your kubeconfig file Try to import any resource to test the connectivity with the Harvester cluster For instance, try to import ssh-key with: terraformer import harvester -r ssh_key  Expected Results You should see:
terraformer import harvester -r ssh_key 2021/08/04 15:18:59 harvester importing... ssh_key 2021/08/04 15:18:59 harvester done importing ssh_key ... And the generated files should appear in ./generated/harvester/ssh_key/</description>
    </item>
    
    <item>
      <title>Check VM creation required-fields</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1283-vm-creation-required-fields/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1283-vm-creation-required-fields/</guid>
      <description> Related issues: #1283 Fix required fields on VM creation page  Verification Steps  Create VM without image name and size Create VM without size Create VM wihout image name Create VM without hostname  Expected Results  You should get an error trying to create VM without image name and size You should get an error trying to create VM without image name You should get an error trying to create VM without size You should not get an error trying to create a VM without hostname  </description>
    </item>
    
    <item>
      <title>Clone VM and don&#39;t select start after creation</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-and-dont-select-start-after-creation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-and-dont-select-start-after-creation/</guid>
      <description> Clone VM from Virtual Machine list  Expected Results  Machine should start if start VM after creation was checked Machine should match the origin machine in Config In YAML You should be able to connect to new VM via console  </description>
    </item>
    
    <item>
      <title>Clone VM that is turned off</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-off/</guid>
      <description> Clone VM from Virtual Machine list that is turned off  Expected Results  Machine should start if start VM after creation was checked Machine should match the origin machine  in Config In YAML   You should be able to connect to new VM via console  </description>
    </item>
    
    <item>
      <title>Clone VM that is turned on</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-is-turned-on/</guid>
      <description> Clone VM from Virtual Machine list that is turned on  Expected Results  Machine should start if start VM after creation was checked Machine should match the origin machine  in Config In YAML   You should be able to connect to new VM via console  </description>
    </item>
    
    <item>
      <title>Clone VM that was created from image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-image/</guid>
      <description> Clone VM from Virtual Machine list  Expected Results  Machine should start if start VM after creation was checked Machine should match the origin machine  in Config In YAML   You should be able to connect to new VM via console  </description>
    </item>
    
    <item>
      <title>Clone VM that was created from template</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-created-from-template/</guid>
      <description> Clone VM from Virtual Machine list  Expected Results  Machine should start if start VM after creation was checked Machine should match the origin machine  in Config In YAML   You should be able to connect to new VM via console  </description>
    </item>
    
    <item>
      <title>Clone VM that was not created from image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-not-created-from-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/clone-vm-that-was-not-created-from-image/</guid>
      <description> Clone VM from Virtual Machine list  Expected Results  Machine should start if start VM after creation was checked Machine should match the origin machine  in Config In YAML   You should be able to connect to new VM via console  </description>
    </item>
    
    <item>
      <title>Cluster add labs</title>
      <link>https://harvester.github.io/tests/manual/node-driver/create-add-labs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/create-add-labs/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster  Expected Results  Use the command &amp;ldquo;kubectl get node &amp;ndash;show-labels&amp;rdquo; to see the success of the added tabs Go to the node details page of UI, click the &amp;ldquo;Edit Node&amp;rdquo; button, and check Labels  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Cluster add Taints</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-add-taints/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-add-taints/</guid>
      <description>add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster  Expected Results  Use the command kubectl describe node test-tain5 | grep Taint to see if Taint was added successfully. Go to the node details page of UI, click the &amp;ldquo;Edit Node&amp;rdquo; button, and check Taint  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Cluster TLS customization</title>
      <link>https://harvester.github.io/tests/manual/_incoming/tls_customize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/tls_customize/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1046
Verify Items  Cluster&amp;rsquo;s SSL/TLS parameters could be configured in install option Cluster&amp;rsquo;s SSL/TLS parameters could be updated in dashboard  Case: Configure TLS parameters in dashboard  Install Harvester with any nodes Navigate to Advanced Settings, then edit ssl-parameters Select Protocols TLSv1.3, then save execute command echo QUIT | openssl s_client -connect &amp;lt;VIP&amp;gt;:443 -tls1_2 | grep &amp;quot;Cipher is&amp;quot; Output should contain error...SSL routines... and Cipher is (NONE) execute command echo QUIT | openssl s_client -connect &amp;lt;VIP&amp;gt;:443 -tls1_3 | grep &amp;quot;Cipher is&amp;quot; Output should contain Cipher is &amp;lt;one_of_TLS1_3_Ciphers&amp;gt;1 and should not contain error.</description>
    </item>
    
    <item>
      <title>CPU overcommit on VM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/cpu_overcommit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/cpu_overcommit/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1429
Verify Items  Overcommit can be edit on Dashboard VM can allocate exceed CPU on the host Node VM can chage allocated CPU after created  Case: Update Overcommit configuration  Install Harvester with any Node Login to Dashboard, then navigate to Advanced Settings Edit overcommit-config The field of CPU should be editable Created VM can allocate maximum CPU should be &amp;lt;HostCPUs&amp;gt; * [&amp;lt;overcommit-CPU&amp;gt;/100] - &amp;lt;Host Reserved&amp;gt;  Case: VM can allocate CPUs more than Host have  Install Harvester with any Node Create a cloud image for VM Creation Create a VM with &amp;lt;HostCPUs&amp;gt; * 5 CPUs VM should start successfully lscpu in VM should display allocated CPUs Page of Virtual Machines should display allocated CPUs correctly  Case: Update VM allocated CPUs  Install Harvester with any Node Create a cloud image for VM Creation Create a VM with &amp;lt;HostCPUs&amp;gt; * 5 CPUs VM should start successfully Increase/Reduce VM allocated CPUs to minimum/maximum VM should start successfully lscpu in VM should display allocated CPUs Page of Virtual Machines should display allocated CPUs correctly  </description>
    </item>
    
    <item>
      <title>Create a 3 nodes harvester cluster with RKE1 (only with mandatory info, other values stays with default)</title>
      <link>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke1/</guid>
      <description> From the Rancher home page, click on Create Select RKE1 on the right and click on Harvester Enter a cluster name Give a prefix name for the VMs Increase count to 3 nodes Check etcd, Control Plane and Worker boxes Select or create a node template if needed  Click on Add node template Create credentials by selecting your harvester cluster Fill the instance option fields, pay attention to correctly write the default ssh user of the chosen image in the SSH user field Give a name to the rancher template and click on Create   Click on create to spin the cluster up  Expected Results  The status of the created cluster shows active The status of the corresponding vm on harvester active The 3 nodes should be with the active status  </description>
    </item>
    
    <item>
      <title>Create a 3 nodes harvester cluster with RKE2 (only with mandatory info, other values stays with default)</title>
      <link>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/create-3-node-rke2/</guid>
      <description> From the Rancher home page, click on Create Select RKE2 on the right and click on Harvester Create the credential to talk with the harvester provider  Select your harvester cluster (external or internal)   Enter a cluster name Increase machine count to 3 Fill the mandatory fields  Namespace Image Network SSH User (default ssh user of the chosen image)   Click on create to spin the cluster up  Expected Results  The status of the created cluster shows active The status of the corresponding vm on harvester active The 3 nodes should be with the active status  </description>
    </item>
    
    <item>
      <title>Create a harvester cluster and add Taint to a node</title>
      <link>https://harvester.github.io/tests/manual/node-driver/q-cluster-add-taint/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/q-cluster-add-taint/</guid>
      <description>Expected Results </description>
    </item>
    
    <item>
      <title>Create a harvester cluster with 3 master nodes</title>
      <link>https://harvester.github.io/tests/manual/node-driver/add-3-master-nodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/add-3-master-nodes/</guid>
      <description> add a harvester node template Create harvester cluster count set to 3  Expected Results  The status of the created cluster shows active show the 3 created node status running in harvester&amp;rsquo;s vm list the information displayed on rancher and harvester matches the template configuration  </description>
    </item>
    
    <item>
      <title>Create a harvester cluster with a non-default version of k8s</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-k8s/</guid>
      <description> Verify versions 1.19.10, 1.18.18, 1.17.17, 1.16.15 respectively  Expected Results  k8s displayed on the UI is consistent with the created version (cluster list, host list) Use kubectl version to see that the version information is the same as the created version  </description>
    </item>
    
    <item>
      <title>Create a harvester cluster with different images</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-different-images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-different-images/</guid>
      <description>d a harvester node template Set the image, it should be a drop-down list, refer to &amp;ldquo;Test Data&amp;rdquo; for other values  ubuntu-18.04-server-cloudimg-amd64.img focal-server-cloudimg-amd64-disk-kvm.img   Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active The status of the corresponding vm on harvester active The information displayed on rancher and harvester matches the template configuration The drop-down list of images in the harvester node template corresponds to the list of images in the harvester  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Create a harvester cluster, template drop-down list validation</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-template-dropdown-multi-user/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-template-dropdown-multi-user/</guid>
      <description> Create multiple harvester Node Templates with different users Add harvester cluster and set Template  Expected Results  pop up a template list pop-up box Show the templates you created and the templates created by other users  </description>
    </item>
    
    <item>
      <title>Create a new VM and add Enable USB tablet option	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-enable-usb-tablet-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-enable-usb-tablet-option/</guid>
      <description> Add Enable usb tablet Option Save/Create VM  Expected Results  Machine starts successfully Enable usb tablet shows  In YAML In Form    </description>
    </item>
    
    <item>
      <title>Create a new VM and add Install guest agent option (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-install-guest-agent-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-and-add-install-guest-agent-option/</guid>
      <description> Add install Guest Agent Option Save/Create VM Validate that qemu-guest-agent was installed  You can do this on ubuntu with the command dpkg -l | grep qemu    Expected Results  Machine starts successfully Guest Agent Option shows  In YAML In Form   Guest Agent is installed  </description>
    </item>
    
    <item>
      <title>Create a new VM with Network Data from the form</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-the-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-the-form/</guid>
      <description> Add Network Data to the VM  Here is an example of Network Data config to add DHCP to the physical interface eth0 network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp    Save/Create the VM  Expected Results  Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0  </description>
    </item>
    
    <item>
      <title>Create a new VM with Network Data from YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-network-data-from-yaml/</guid>
      <description> Add Network Data to the VM via YAML  Here is an example of Network Data config to add DHCP to the physical interface eth0 network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp    Save/Create the VM  Expected Results  Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0  </description>
    </item>
    
    <item>
      <title>Create a new VM with User Data from the form</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-user-data-from-the-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-new-vm-with-user-data-from-the-form/</guid>
      <description> Add User data to the VM   Here is an example of user data config to add a password #cloud-config password: password chpasswd: {expire: False} sshpwauth: True Save/Create the VM  Expected Results  Machine starts succesfully User data should exist  In YAML In Form   Machine should have user password set  </description>
    </item>
    
    <item>
      <title>Create a VM on a VLAN with an existing machine and then change the existing machine&#39;s VLAN</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-on-a-vlan-with-an-existing-machine-and-then-change-the-existing-machines-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-on-a-vlan-with-an-existing-machine-and-then-change-the-existing-machines-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate  Expected Results  VM should create successfully Appropriate VLAN should show  In config in YAML   VMs should NOT be able to connect on network  verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server    </description>
    </item>
    
    <item>
      <title>Create a VM through the Rancher dashboard</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1613-create-vm-through-rancher-dashboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1613-create-vm-through-rancher-dashboard/</guid>
      <description> Related issues: #1613 VM memory shows NaN Gi  Verification Steps  import harvester into rancher&amp;rsquo;s virtualization management Load Harvester dashboard by going to virtualization management then clicking on harvester cluster Create a new VM on Harvester Validate the following in the VM list page, the form, and YAML&amp;gt;  Memory CPU Disk space    Expected Results  VM should create VM should start All specifications should show correctly  </description>
    </item>
    
    <item>
      <title>Create a VM with 2 networks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-2-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-2-networks/</guid>
      <description>Add a network to the VM Save the VM Wait for it to start/restart  Expected Results  the VM should start successfully The already existing network connectivity should still work The new connectivity should also work  Comments one default management network and one VLAN</description>
    </item>
    
    <item>
      <title>Create a vm with all the default values (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-all-the-default-values/</guid>
      <description> Create a VM with all default values Save  Expected Results  VM should save VM should start if start after creation checkbox is checked Config should show  In Form In YAML    </description>
    </item>
    
    <item>
      <title>Create a VM with Start VM on Creation checked (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-checked/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-checked/</guid>
      <description> Create VM  Expected Results  VM should start Checkbox for start virtual machine on creation should show as appropriate while editing machine after creation  </description>
    </item>
    
    <item>
      <title>Create a VM with start VM on creation unchecked (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-a-vm-with-start-vm-on-creation-unchecked/</guid>
      <description> Create VM  Expected Results  VM should start or not start as appropriate Checkbox for start virtual machine on creation should show as appropriate while editing machine after creation  </description>
    </item>
    
    <item>
      <title>Create Backup Target (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/create-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/create-backup-target/</guid>
      <description> Open up Backup-target in settings Input server info Save  Expected Results  Backup Target should show in settings  </description>
    </item>
    
    <item>
      <title>Create harvester cluster using non-default CPUs, Memory, Disk</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-non-default-resources/</guid>
      <description>add a harvester node template The set CPUs, Memory, and Disk values, refer to &amp;ldquo;Test Data&amp;rdquo; for other values Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:4 Memorys:8 Disk:50 Bus:Virtlo Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Create harvester clusters with different Bus</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-different-bus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-different-bus/</guid>
      <description>add a harvester node template Set the “Network Name”, it should be a drop-down list, refer to &amp;ldquo;Test Data&amp;rdquo; for other values  VirtIO SATA SCSI   Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration The drop-down list of &amp;ldquo;BUS&amp;rdquo; in the harvester node template corresponds to the list of “BUS” in the harvester  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Create harvester clusters with different Networks</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-different-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-different-networks/</guid>
      <description>add a harvester node template Set the “Network Name”, it should be a drop-down list, refer to &amp;ldquo;Test Data&amp;rdquo; for other values  vlan1 vlan2   Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration The drop-down list of &amp;ldquo;Network Name&amp;rdquo; in the harvester node template corresponds to the list of “Network Name” in the harvester  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Create image from Volume</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-image-from-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/create-image-from-volume/</guid>
      <description>Create new VM Add SSH key Run through iterations for 1, 2, and 3 for attached bash script Export volume to image from volumes page Create new VM from image Run md5sum -c file2.md5 file1-2.md5 file2-2.md5 file3.md5  Expected Results  image should upload/complete in images page New VM should create SSH key should work on new VM file2.md5 should fail and the other three md5 checks should pass  Comments #!</description>
    </item>
    
    <item>
      <title>Create Images with valid image URL (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/create-images-with-valid-image-url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/create-images-with-valid-image-url/</guid>
      <description>Create image with cloud image available for openSUSE. http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2  Expected Results  Image should show state as Active. Check the backing image in Longhorn  Known Bugs https://github.com/harvester/harvester/issues/1269</description>
    </item>
    
    <item>
      <title>Create multiple instances of the vm with ISO image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-iso-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-iso-image/</guid>
      <description>Create images using the external path for ISO image. In user data mention the below to access the vm.  Create the 3 vms and wait for vm to start  Expected Results  3 vm should come up and start with same config. Observe the time taken for the system to start the vms. Observe the pattern of the vms get allocated on the nodes. Like how many vm on each nodes are created.</description>
    </item>
    
    <item>
      <title>Create multiple instances of the vm with raw image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-raw-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-raw-image/</guid>
      <description>Create images using the external path for cloud image. In user data mention the below to access the vm.  Create the 3 vms and wait for vm to start.  Expected Results  3 vm should come up and start with same config. Observe the time taken for the system to start the vms. Observe the pattern of the vms get allocated on the nodes. Like how many vm on each nodes are created.</description>
    </item>
    
    <item>
      <title>Create multiple instances of the vm with Windows Image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-windows-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-multiple-instances-vm-with-windows-image/</guid>
      <description>Create images using the external path for ISO image. In user data mention the below to access the vm.  Create the 3 vms and wait for vm to start.  Expected Results  3 vm should come up and start with same config. Observe the time taken for the system to start the vms. Observe the pattern of the vms get allocated on the nodes. Like how many vm on each nodes are created.</description>
    </item>
    
    <item>
      <title>Create new network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/create-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/create-network/</guid>
      <description> Navigate to the networks page in harvester Click Create Add a name Add a VLAN ID Click Create  Expected Results  You should be able to add the VLAN You should see the VLAN show up in the networks page  </description>
    </item>
    
    <item>
      <title>Create new VM with a machine type of PC (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-pc/</guid>
      <description> Set up the VM with the appropriate machine type Save/create  Expected Results  Machine should start sucessfully Machine should show the new machine type in the config and in the YAML  </description>
    </item>
    
    <item>
      <title>Create new VM with a machine type of q35 (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-new-vm-with-a-machine-type-q35/</guid>
      <description> Set up the VM with the appropriate machine type Save/create  Expected Results  Machine should start sucessfully Machine should show the new machine type in the config and in the YAML  </description>
    </item>
    
    <item>
      <title>Create one VM on a VLAN and then move another VM to that VLAN</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-and-then-move-another-vm-to-that-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-and-then-move-another-vm-to-that-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN  Expected Results  VM should create successfully Appropriate VLAN should show  In config in YAML   VMs should be able to connect on network  This can be verified with a ping over the IP, or via other options if ICMP is disabled    </description>
    </item>
    
    <item>
      <title>Create one VM on a VLAN that has other VMs then change it to a different VLAN</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-that-has-other-vms-then-change-it-to-a-different-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-one-vm-on-a-vlan-that-has-other-vms-then-change-it-to-a-different-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate  Expected Results  VM should create successfully Appropriate VLAN should show  In config in YAML   VMs should NOT be able to connect on network  verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server    </description>
    </item>
    
    <item>
      <title>Create RKE2 cluster with no cloud provider</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1577-create-rke2-cluster-no-cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1577-create-rke2-cluster-no-cloud-provider/</guid>
      <description> Related issues: #1577 Option to disable load balancer feature in cloud provider  Verification Steps  Click Cluster Management Click Cloud Credentials Click createa and select Harvester Input credential name Select existing cluster in the Imprted Cluster list Click Create   Click Clusters Click Create Toggle RKE2/K3s Select Harvester Input Cluster Name Select default namespace Select ubuntu image Select network vlan1 Input SSH User: ubuntu Select None for cloud provider  Click Create   Wait for RKE2 cluster provisioning complete (~20min)  Expected Results  Provision RKE2 cluster successfully with Running status   Can acccess RKE2 cluster to check all resources and services by clicking manage  </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with ISO image</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-pc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-pc/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm.  #cloud-config password: password chpasswd: {expire: False} sshpwauth: True  Create the vm and wait for vm to start.  Expected Results  VM should come up and start with same config.  </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with ISO image	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm.  #cloud-config password: password chpasswd: {expire: False} sshpwauth: True  Create the vm and wait for vm to start.  Expected Results  VM should come up and start with same config.  </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with ISO image with machine type pc</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-q35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-iso-image-with-machine-type-q35/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm.  #cloud-config password: password chpasswd: {expire: False} sshpwauth: True  Create the vm and wait for vm to start.  Expected Results  VM should come up and start with same config.  </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with raw image	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-raw-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-raw-image/</guid>
      <description> Create vm using the external path for cloud image. In user data mention the below to access the vm.  #cloud-config password: password chpasswd: {expire: False} sshpwauth: True  Create the vm and wait for vm to start.  Expected Results  VM should come up and start with same config.  </description>
    </item>
    
    <item>
      <title>Create Single instances of the vm with Windows Image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-windows-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-single-instances-vm-with-windows-image/</guid>
      <description> Create vm using the external path for ISO image. In user data mention the below to access the vm.  #cloud-config password: password chpasswd: {expire: False} sshpwauth: True  Create the vm and wait for vm to start.  Expected Results  VM should come up and start with same config.  </description>
    </item>
    
    <item>
      <title>Create SSH key from templates page</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1619-create-ssh-key-from-templates-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1619-create-ssh-key-from-templates-page/</guid>
      <description> Related issues: #1619 User is unable to create ssh key through the templates page  Verification Steps  on a harvester deployment, navigate to advanced -&amp;gt; templates and click create Click create new under SSH section enter valid credentials and save  Expected Results  SSH key should be created and show in the SSH key section  </description>
    </item>
    
    <item>
      <title>Create support bundle in multi-node Harvester cluster with one node off</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1524-create-support-bundle-with-one-node-off/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1524-create-support-bundle-with-one-node-off/</guid>
      <description> Related issues: #1524 Can&amp;rsquo;t create support bundle if one node is off  Verification Steps  On a multi-node harvester cluster power off one node Navigate to support create support bundle  Expected Results  Support bundle should create and be downloaded YOu should be able to extract and examine support bundle  </description>
    </item>
    
    <item>
      <title>Create two VMs in the same VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-in-the-same-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-in-the-same-vlan/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN  Expected Results  VM should create successfully Appropriate VLAN should show  In config in YAML   VMs should be able to connect on network  This can be verified with a ping over the IP, or via other options if ICMP is disabled    </description>
    </item>
    
    <item>
      <title>Create two VMs on separate VLANs</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-separate-vlans/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-separate-vlans/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate  Expected Results  VM should create successfully Appropriate VLAN should show  In config in YAML   VMs should NOT be able to connect on network  verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server    </description>
    </item>
    
    <item>
      <title>Create two VMs on the same VLAN and change one</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-the-same-vlan-and-change-one/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-two-vms-on-the-same-vlan-and-change-one/</guid>
      <description> Create/edit VM/VMs with the appropriate VLAN Change VLAN for VM if appropriate  Expected Results  VM should create successfully Appropriate VLAN should show  In config in YAML   VMs should NOT be able to connect on network  verify with ping/ICMP verify with SSH verify with telnet over port 80 if there&amp;rsquo;s a web server    </description>
    </item>
    
    <item>
      <title>Create VM and add SSH key (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-and-add-ssh-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-and-add-ssh-key/</guid>
      <description> Create VM Add SSH key if not already in VM Logon with SSH  Expected Results  You should be prompted for SSH key passphrase if appropriate You should connect You should be able to execute shell commands The SSH Key should show in the SSH key list  </description>
    </item>
    
    <item>
      <title>Create vm using a template of default version</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version/</guid>
      <description> Create a new VM with a template of default version  Expected Results  After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected  </description>
    </item>
    
    <item>
      <title>Create vm using a template of default version with machine type pc</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-pc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-pc/</guid>
      <description> Create a new VM with a template of default version  Expected Results  After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected  </description>
    </item>
    
    <item>
      <title>Create vm using a template of default version with machine type q35</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-q35/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-of-default-version-with-machine-type-q35/</guid>
      <description> Create a new VM with a template of default version  Expected Results  After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected  </description>
    </item>
    
    <item>
      <title>Create vm using a template of non-default version</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-non-default-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-using-a-template-non-default-version/</guid>
      <description> Create a new VM with a template of non-default version  Expected Results  After selecting appropriate template and/or version it should populate other fields CPU, Memory, Image, and SSH key should match saved template info VM should start after creation if Start Virtual Machine is selected  </description>
    </item>
    
    <item>
      <title>Create vm with both CPU and Memory not in cluster (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-both-cpu-and-memory-not-in-cluster/</guid>
      <description> Attempt to create a VM with the appropriate resources  Expected Results  You should get errors for each resource you over provisioned The VM should not create until errors are resolved  </description>
    </item>
    
    <item>
      <title>Create vm with CPU not in cluster. (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-cpu-not-in-cluster/</guid>
      <description> Attempt to create a VM with the appropriate resources  Expected Results  You should get errors for each resource you over provisioned The VM should not create until errors are resolved  </description>
    </item>
    
    <item>
      <title>Create VM with existing Volume (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-existing-volume/</guid>
      <description> Create VM with an existing volume  Expected Results  VM should create and start You should be able to open the console for the VM and see it boot Volume should show in volumes list VM should appear to the &amp;ldquo;Attached VM&amp;rdquo; column of the existing volume  </description>
    </item>
    
    <item>
      <title>Create vm with Memory not in cluster. (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-memory-not-in-cluster/</guid>
      <description> Attempt to create a VM with the appropriate resources  Expected Results  You should get errors for each resource you over provisioned The VM should not create until errors are resolved  </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster CPU</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</guid>
      <description> Edit a VM with resources that are only available on one node in cluster.  Expected Results  VM should save VM should be reassigned to node that has available resources VM should boot VM should pass health checks  </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster CPU (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu/</guid>
      <description> Create a VM with resources that are only available on one node in cluster  Expected Results  VM should create VM should be assigned to node that has available resources VM should boot VM should pass health checks  </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster CPU and Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</guid>
      <description> Create a VM with resources that are only available on one node in cluster  Expected Results  VM should create VM should be assigned to node that has available resources VM should boot VM should pass health checks  </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</guid>
      <description> Edit a VM with resources that are only available on one node in cluster.  Expected Results  VM should save VM should be reassigned to node that has available resources VM should boot VM should pass health checks  </description>
    </item>
    
    <item>
      <title>Create VM with resources that are only on one node in cluster Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-resources-that-are-only-on-one-node-in-cluster-memory/</guid>
      <description> Create a VM with resources that are only available on one node in cluster  Expected Results  VM should create VM should be assigned to node that has available resources VM should boot VM should pass health checks  </description>
    </item>
    
    <item>
      <title>Create VM with saved SSH key (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-saved-ssh-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-saved-ssh-key/</guid>
      <description> Create VM Add SSH key if not already in VM Logon with SSH  Expected Results  You should be prompted for SSH key passphrase if appropriate You should connect You should be able to execute shell commands The SSH Key should show in the SSH key list  </description>
    </item>
    
    <item>
      <title>Create VM with the default network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-the-default-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-the-default-network/</guid>
      <description> Create a VM with the default network Let VM boot up after creation  Expected Results  VM should start VM should be able to ping other machines in the VLAN VM should be able to ping servers on the internet if the VLAN has external access  </description>
    </item>
    
    <item>
      <title>Create VM with two disk volumes (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-vm-with-two-disk-volumes/</guid>
      <description> Create a VM with the appropriate number of volumes  Expected Results  Verify after creation that the appropriate volumes are in the config for the VM Verify that the volumes are created and listed in the volumes section  </description>
    </item>
    
    <item>
      <title>Create VM without memory provided</title>
      <link>https://harvester.github.io/tests/manual/_incoming/create-vm-without-memory-provided/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/create-vm-without-memory-provided/</guid>
      <description>Related issues: #1477 intimidating error message when missing mandatory field  Category:  Virtual Machine  Verification Steps  Create some image and volume Create virtual machine Fill out all mandatory field but leave memory blank. Click create  Expected Results Leave empty memory field empty when create virtual machine will show &amp;ldquo;Memory is required&amp;rdquo; error message</description>
    </item>
    
    <item>
      <title>Create Volume root disk blank Form with label</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-blank-form-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-blank-form-label/</guid>
      <description> Navigate to volumes page Click Create Don&amp;rsquo;t select an image Input a size Click Create  Expected Results  Page should load Volume should create successfully and go to succeeded in the list The label can be seen when you edit the volume config  </description>
    </item>
    
    <item>
      <title>Create volume root disk VM Image Form</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form/</guid>
      <description> Navigate to volumes page Click Create Select an image Input a size Click Create  Expected Results  VM should create VM should pass health checks  </description>
    </item>
    
    <item>
      <title>Create volume root disk VM Image Form with label (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/create-volume-root-disk-vm-image-form-label/</guid>
      <description> Navigate to volumes page Click Create Select an image Input a size Click Create  Expected Results  Page should load Volume should create successfully and go to succeeded in the list The label can be seen when you edit the volume config  </description>
    </item>
    
    <item>
      <title>Create Windows VM</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/create-windows-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/create-windows-vm/</guid>
      <description>Create a VM with the VM template with windows-iso-image-base-temp Config the CPU and Memory to 4 and 8 respectively Select the windows ISO image Click the Volumes tab and update the root disk size to 50GB Click create to launch the windows VM Optional: you can increase the second disk size or add an additional one. Click create to launch the VM (this will take a couple of minutes upon your network speed of download the ISO image) Click the Console to launch a VNC console of the windows server, and you will need to find an evaluation key of the windows server 2012 installation.</description>
    </item>
    
    <item>
      <title>Create with invalid image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/negative-create-with-invalid-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/negative-create-with-invalid-image/</guid>
      <description> Create image with invalid URL. e.g. - https://test.img  Expected Results  Image state show as Failed  </description>
    </item>
    
    <item>
      <title>datavolumes.cdi.kubevirt.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/datavolumes.cdi.kubevirt.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/datavolumes.cdi.kubevirt.io/</guid>
      <description>GUI  Create a VM in GUI and wait until it&amp;rsquo;s running. Assume its name is test.  kube-api  Try to delete its datavolume:  $ kubectl get vms NAME AGE STATUS READY test 5m16s Running True  There should be an datavolume bound to that VM  $ kubectl get dvs NAME PHASE PROGRESS RESTARTS AGE test-disk-0-klrft Succeeded 100.0% 5m18s  The user should not be able to delete the datavolume  $ kubectl delete dv test-disk-0-klrft The request is invalid: : can not delete the volume test-disk-0-klrft which is currently attached to VMs: default/test `` ## Expected Results ### kube-api The deletion of its datavolume should fail.</description>
    </item>
    
    <item>
      <title>Deactivate/activate/delete Harvester Node Driver</title>
      <link>https://harvester.github.io/tests/manual/node-driver/deactivate-activate-deletenode-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/deactivate-activate-deletenode-driver/</guid>
      <description>With Rancher &amp;lt; 2.6:
 Tools-&amp;gt;Driver Management→Node Driver Deactivate/activate/delete Harvester Node Driver With Rancher 2.6: Cluster Management &amp;gt; Drivers &amp;gt; Node Drivers Deactivate/activate/delete Harvester Node Driver  Expected Results  Harvester icon is not visible when creating a cluster / Harvester icon is visible when creating a cluster /Harvester icon is not visible when creating a cluster  </description>
    </item>
    
    <item>
      <title>Delete 3 node RKE2 cluster</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1311-delete-3-node-rke2-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1311-delete-3-node-rke2-cluster/</guid>
      <description> Related issues: #1311 Deleting a cluster in rancher dashboard doesn&amp;rsquo;t fully remove  Verification Steps  Create 3 node RKE2 cluster on Harvester through node driver with Rancher Wait fo the nodes to create, but not fully provision Delete the cluster Wait for them to be removed from Harvester Check Rancher cluster management  Expected Results  Cluster should be removed from Rancher VMs should be removed from Harvester  </description>
    </item>
    
    <item>
      <title>Delete backup from backups list (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-single-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-single-backup/</guid>
      <description> Delete backup from backups list  Expected Results  Backup should be removed from list Backup should be removed from remote storage  </description>
    </item>
    
    <item>
      <title>Delete Cluster</title>
      <link>https://harvester.github.io/tests/manual/node-driver/cluster-delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/cluster-delete/</guid>
      <description> Delete Cluster  Expected Results  successful cluster deletion in rancher the corresponding VM node in harvester is deleted successfully  </description>
    </item>
    
    <item>
      <title>Delete external VLAN network via form</title>
      <link>https://harvester.github.io/tests/manual/network/delete-vlan-network-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/delete-vlan-network-form/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the external VLAN via the web form Validate interface was removed with  ip link list   Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine  Expected Results  The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the external VLAN You should get responses from the VM  </description>
    </item>
    
    <item>
      <title>Delete external VLAN network via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/delete-vlan-network-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/delete-vlan-network-yaml/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the external VLAN via YAML Validate interface was removed with  ip link list   Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine  Expected Results  The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the external VLAN You should get responses from the VM  </description>
    </item>
    
    <item>
      <title>Delete first backup in chained backup (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-first-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-first-backup-chained-backup/</guid>
      <description> Create a new VM Create a file named 1 and add text Create a backup Edit text in file 1 create file 2 Create Backup Edit file 2 text Create file 3 and add text Create backup Delete backup 1 Validate file 2 and 3 are the same as they were Restore to backup 2 Validate that  md5sum -c file1-2.md5 file2.md5 file3.md5 file 1 is in second format file 2 is in first format file 3 doesn&amp;rsquo;t exist    Expected Results  Vm should create All file operations should create Backup should run All file operations should create Backup should run All file operations should create files should be as expected  </description>
    </item>
    
    <item>
      <title>Delete Host (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/delete-host/</guid>
      <description> Navigate to the Hosts page and select the node Click Delete  Expected Results  SSH to the node and check the nodes has components deleted.  </description>
    </item>
    
    <item>
      <title>Delete host that has VMs on it</title>
      <link>https://harvester.github.io/tests/manual/hosts/delete-host-with-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/delete-host-with-vm/</guid>
      <description>Navigate to the Hosts page and select the node Click Delete  Expected Results  An alert message should appear. If VM exists it should stop user to delete the node or move VM to other node. If VM is getting moved to another node and there is no space, it should stop user to delete the node.  Existing bugs https://github.com/harvester/harvester/issues/1004</description>
    </item>
    
    <item>
      <title>Delete last backup in chained backup (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-last-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-last-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup delete backup 3 Validate that files didn&amp;rsquo;t change Restore to backup 2 Validate that  md5sum -c file1-2.</description>
    </item>
    
    <item>
      <title>Delete management network via form</title>
      <link>https://harvester.github.io/tests/manual/network/delete-management-network-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/delete-management-network-form/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the management VLAN via the web form Validate interface was removed with  ip link list   Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine  Expected Results  The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the management VLAN You should get responses from the VM  </description>
    </item>
    
    <item>
      <title>Delete management network via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/delete-management-network-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/delete-management-network-yaml/</guid>
      <description> On a VM with both an external VLAN and a management VLAN delete the management network via YAML Validate interface was removed with  ip link list   Ping the VM from another VM that is only on the management VLAN Ping the VM from an external machine  Expected Results  The VM should update and reboot You should only see one interface (and the loopback) in the list You should not be able to ping the VM on the management network You should get responses from the VM  </description>
    </item>
    
    <item>
      <title>Delete middle backup in chained backup (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-middle-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-middle-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Delete backup 2 Validate file 2 and 3 are the same as they were Restore to backup 1 Validate that  md5sum -c file1.</description>
    </item>
    
    <item>
      <title>Delete multiple backups</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/delete-multiple-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/delete-multiple-backups/</guid>
      <description> Select multiple Backups from Backups list Click Delete  Expected Results  Backups should be removed from list Backups should be removed from remote storage  </description>
    </item>
    
    <item>
      <title>Delete multiple VMs with disks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-with-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-with-disks/</guid>
      <description> Delete VM Select whether you want to delete disks  Expected Results  You should check amount of used space on Server before you delete the VM Machine should delete It should not show up in the Virtual Machine list Disks should be listed/or not in Volumes list as appropriate Verify the cleaned up the space on the disk on the node.  </description>
    </item>
    
    <item>
      <title>Delete multiple VMs without disks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-without-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/delete-multiple-vms-without-disks/</guid>
      <description> Delete VM Select whether you want to delete disks  Expected Results  You should check amount of used space on Server before you delete the VM Machine should delete It should not show up in the Virtual Machine list Disks should be listed/or not in Volumes list as appropriate Verify the cleaned up the space on the disk on the node.  </description>
    </item>
    
    <item>
      <title>Delete single vm all disks (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/delete-single-vm-all-disks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/delete-single-vm-all-disks/</guid>
      <description> Delete VM Select whether you want to delete disks  Expected Results  You should check amount of used space on Server before you delete the VM Machine should delete It should not show up in the Virtual Machine list Disks should be listed/or not in Volumes list as appropriate Verify the cleaned up the space on the disk on the node.  </description>
    </item>
    
    <item>
      <title>Delete the image	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/delete-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/delete-image/</guid>
      <description> Select an image with state active. Delete the image. Create another image with same name. Delete the newly created image. Delete an image with failed state  Expected Results  The image should be deleted successfully. Check the CRDS VirtualMachineImage. User should be able to create a new image with same name. Check the backing image in Longhorn.  </description>
    </item>
    
    <item>
      <title>Delete VM Negative (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-delete-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-delete-vm/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Delete VM and all disks  Expected Results  You should not be able to delete the VM  </description>
    </item>
    
    <item>
      <title>Delete VM with exported image</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1602-delete-vm-with-exported-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1602-delete-vm-with-exported-image/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed  Verification Steps  create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; delete image &amp;ldquo;img-1&amp;rdquo;  Expected Results  image &amp;ldquo;img-1&amp;rdquo; will be deleted  </description>
    </item>
    
    <item>
      <title>Delete volume that is not attached to a VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/delete-volume-that-is-not-attached-to-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/delete-volume-that-is-not-attached-to-vm/</guid>
      <description> Create volume Validate that it created Check the volume crd. Delete the volume Verify that volume is removed from list Check the volume object doesn&amp;rsquo;t exist anymore.  Expected Results  Volume should create It should show in volume list Volume crd should have correct info. Volume should delete. Volume should be removed from list  </description>
    </item>
    
    <item>
      <title>Delete volume that was attached to VM but now is not (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/delete-volume-that-was-attached-to-vm-but-is-not-now/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/delete-volume-that-was-attached-to-vm-but-is-not-now/</guid>
      <description> Create a VM with a root volume Write 10Gi data into it. Delete the VM but not the volume Verify Volume still exists Check disk space on node Delete the volume Verify that volume is removed from list Check disk space on node  Expected Results  VM should create 10Gi space should be consumed on the disk. VM should delete Volume should still show in Volume list Disk space should show 10Gi + Volume should delete Volume should be removed from list Space should be less than before  </description>
    </item>
    
    <item>
      <title>Detach volume from virtual machine</title>
      <link>https://harvester.github.io/tests/manual/_incoming/detach-volume-from-virtual-machine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/detach-volume-from-virtual-machine/</guid>
      <description>Related issues: #1708 After click &amp;ldquo;Detach volume&amp;rdquo; button, nothing happend  Category:  Volume  Verification Steps  Create several new volume in volumes page  Create a virtual machine Click the config button on the selected virtual machine Click Add volume and add at least two new volume  Click the Detach volume button on the attached volume    Repeat above steps several times  Expected Results Currently when click the Detach volume button, attached volume can be detach successfully.</description>
    </item>
    
    <item>
      <title>Disable and enable vlan cluster network</title>
      <link>https://harvester.github.io/tests/manual/_incoming/disable-and-enable-vlan-cluster-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/disable-and-enable-vlan-cluster-network/</guid>
      <description> Related issues: #1529 Failed to enable vlan cluster network after disable and enable again, display &amp;ldquo;Network Error&amp;rdquo;  Category:  Network  Verification Steps  Open settings and config vlan network Enable network and set default harvester-mgmt Disable network Enable network again Check Host, Network and harvester dashboard Repeat above steps several times  Expected Results  User can disable and enable network with default harvester-mgmt. Harvester dashboard and network work as expected  </description>
    </item>
    
    <item>
      <title>Disk can only be added once on UI</title>
      <link>https://harvester.github.io/tests/manual/_incoming/add_disk_on_ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/add_disk_on_ui/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1608
Verify Items  NVMe disk can only be added once on UI  Case: add new NVMe disk on dashboard UI  Install Harvester with 2 nodes Power off 2nd node Update VM&amp;rsquo;s xml definition (by using virsh edit or virt-manager)  Create nvme.img block: dd if=/dev/zero of=/var/lib/libvirt/images/nvme.img bs=1M count=4096 change owner chown qemu:qemu /var/lib/libvirt/images/nvme.img update &amp;lt;domain type=&amp;quot;kvm&amp;quot;&amp;gt; to &amp;lt;domain type=&amp;quot;kvm&amp;quot; xmlns:qemu=&amp;quot;http://libvirt.org/schemas/domain/qemu/1.0&amp;quot;&amp;gt; append xml node into domain as below:    &amp;lt;qemu:commandline&amp;gt; &amp;lt;qemu:arg value=&amp;#34;-drive&amp;#34;/&amp;gt; &amp;lt;qemu:arg value=&amp;#34;file=/var/lib/libvirt/images/nvme.</description>
    </item>
    
    <item>
      <title>Disk devices used for VM storage should be globally configurable</title>
      <link>https://harvester.github.io/tests/manual/_incoming/disk-devices-used-for-vm-storage-globally-configurable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/disk-devices-used-for-vm-storage-globally-configurable/</guid>
      <description>Related issue: #1241 Disk devices used for VM storage should be globally configurable
  Related issue: #1382 Exclude OS root disk and partitions on forced GPT partition
  Related issue: #1599 Extra disk auto provision from installation may cause NDM can&amp;rsquo;t find a valid longhorn node to provision
  Category:  Storage  Test Scenarios (Checked means verification PASS)
 BIOS firmware + No MBR (Default) + Auto disk` provisioning config BIOS firmware + MBR + Auto disk provisioning config UEFI firmware + GPT (Default) + Auto disk provisioning config BIOS firmware + GPT (Default) +Auto Provisioning on harvester-config  Environment setup   Scenario 1: Node type: Create</description>
    </item>
    
    <item>
      <title>Download host YAML</title>
      <link>https://harvester.github.io/tests/manual/hosts/download-host-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/download-host-yaml/</guid>
      <description> Navigate to the Hosts page and select the node Click Download Yaml  Expected Results  The Yaml should get downloaded.  </description>
    </item>
    
    <item>
      <title>Download kubeconfig after shutting down harvester cluster</title>
      <link>https://harvester.github.io/tests/manual/_incoming/download-kubeconfig-after-shutting-down-harvester-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/download-kubeconfig-after-shutting-down-harvester-cluster/</guid>
      <description>Related issues: #1475 After shutting down the cluster the kubeconfig becomes invalid  Category:  Host  Verification Steps   Shutdown harvester node 3, wait for fully power off
  Shutdown harvester node 2, wait for fully power off
  Shutdown harvester node 1, wait for fully power off
  Wait for more than hours or over night
  Power on node 1 to console page until you see management url   Power on node 2 to console page until you see management url</description>
    </item>
    
    <item>
      <title>Edit a VM and add install Enable usb tablet option (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-enable-usb-tablet-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-enable-usb-tablet-option/</guid>
      <description> Add Enable usb tablet Option Save/Create VM  Expected Results  Machine starts successfully Enable usb tablet shows  In YAML In Form    </description>
    </item>
    
    <item>
      <title>Edit a VM and add install guest agent option (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-guest-agent-option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-and-add-install-guest-agent-option/</guid>
      <description> Add install Guest Agent Option Save/Create VM  Expected Results  Machine starts successfully Guest Agent Option shows  In YAML In Form   Guest Agent is installed  </description>
    </item>
    
    <item>
      <title>Edit a VM from the form to add Network Data</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-network-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-network-data/</guid>
      <description> Add Network Data to the VM  Here is an example of Network Data config to add DHCP to the physical interface eth0  network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp  Save/Create the VM  Expected Results  Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0  </description>
    </item>
    
    <item>
      <title>Edit a VM from the form to add user data</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-user-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-form-to-add-user-data/</guid>
      <description> Add User data to the VM  Here is an example of user data config to add a password `` #cloud-config password: password chpasswd: {expire: False} sshpwauth: True   Save/Create the VM  Expected Results  Machine starts succesfully User data should  In YAML In Form   Machine should have user password set  </description>
    </item>
    
    <item>
      <title>Edit a VM from the YAML to add Network Data (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-network-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-network-data/</guid>
      <description> Add Network Data to the VM  Here is an example of Network Data config to add DHCP to the physical interface eth0  network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp  Save/Create the VM  Expected Results  Machine starts succesfully Network Data should show in YAML Network Datashould show in Form Machine should have DHCP for network on eth0  </description>
    </item>
    
    <item>
      <title>Edit a VM from the YAML to add user data (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-user-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-from-the-yaml-to-add-user-data/</guid>
      <description> Add User data to the VM  Here is an example of user data config to add a password `` #cloud-config password: password chpasswd: {expire: False} sshpwauth: True   Save/Create the VM  Expected Results  Machine starts succesfully User data should  In YAML In Form   Machine should have user password set  </description>
    </item>
    
    <item>
      <title>Edit an existing VM to another machine type (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-an-existing-vm-to-another-machine-type/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-an-existing-vm-to-another-machine-type/</guid>
      <description> Set up the VM with the appropriate machine type Save/create  Expected Results  Machine should start sucessfully Machine should show the new machine type in the config and in the YAML  </description>
    </item>
    
    <item>
      <title>Edit backup read YAML from file</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-read-yaml-from-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-read-yaml-from-file/</guid>
      <description> Edit YAML for backup Read from File Show Diff Save  Expected Results  Diff should show changes Backup should be updated  </description>
    </item>
    
    <item>
      <title>Edit backup via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/edit-backup-yaml/</guid>
      <description> Edit YAML for backup Show Diff Save  Expected Results  Diff should show changes Backup should be updated  </description>
    </item>
    
    <item>
      <title>Edit Config (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/edit-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/edit-config/</guid>
      <description> Navigate to the Hosts page and select the node Click edit config. Add description and other details Try to modify the network config  Expected Results  The edited values should be saved and reflected on the page.  </description>
    </item>
    
    <item>
      <title>Edit Config YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/edit-config-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/edit-config-yaml/</guid>
      <description> Navigate to the Hosts page and select the node Click edit config through YAML. Add description and other details Try to modify the network config  Expected Results  The edited values should be saved and reflected on the page.  </description>
    </item>
    
    <item>
      <title>Edit images (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/edit-images/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/edit-images/</guid>
      <description> Edit image.  Try to edit the description Try to edit the URL Try to edit the Labels    Expected Results  User should be able to edit the description and Labels User should not be able to edit the URL  </description>
    </item>
    
    <item>
      <title>Edit network via form change external VLAN to management network</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-form-change-vlan-to-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/edit-network-form-change-vlan-to-management/</guid>
      <description> Edit VM and change external VLAN to management network with bridge type via the web form Ping VM Attempt to SSH to VM  Expected Results  VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM  </description>
    </item>
    
    <item>
      <title>Edit network via form change management network to external VLAN</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/edit-network-form-change-management-to-vlan/</guid>
      <description> Edit VM and change management network to external VLAN with bridge type via the web form Ping VM Attempt to SSH to VM  Expected Results  VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM  </description>
    </item>
    
    <item>
      <title>Edit network via YAML change external VLAN to management network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-vlan-to-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-vlan-to-management/</guid>
      <description> Edit VM and change external VLAN to management network with bridge type via YAML Ping VM Attempt to SSH to VM  Expected Results  VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM  </description>
    </item>
    
    <item>
      <title>Edit network via YAML change management network to external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-management-to-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/edit-network-yaml-change-management-to-vlan/</guid>
      <description> Edit VM and change management network to external VLAN with bridge type via YAML Ping VM Attempt to SSH to VM  Expected Results  VM should save and reboot You should be able to ping the VM from an external network You should be able to SSH to VM  </description>
    </item>
    
    <item>
      <title>Edit vm and insert ssh and check the ssh key is accepted for the login (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-and-insert-ssh-and-check-the-ssh-key-is-accepted-for-the-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-and-insert-ssh-and-check-the-ssh-key-is-accepted-for-the-login/</guid>
      <description> Edit VM and add SSH Key Save VM  Expected Results  You should be able to ssh in with correct SSH private key You should not be able to SSH in with incorrect SSH private key  </description>
    </item>
    
    <item>
      <title>Edit VM Form Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-edit-vm-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-edit-vm-form/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Edit the VM via form Save the VM  Expected Results  You should not be able to save the edited Form You should get an error  </description>
    </item>
    
    <item>
      <title>Edit vm network and verify the network is working as per configuration (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-network-and-verify-the-network-is-working-as-per-configuration-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-network-and-verify-the-network-is-working-as-per-configuration-/</guid>
      <description> Edit VM network Save  Expected Results  VM should save VM should restart if restart checkbox is checked Changes should show  In Form In YAML   Network should function as desired  </description>
    </item>
    
    <item>
      <title>Edit VM via form with CPU</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu/</guid>
      <description> Edit VM Save  Expected Results  VM should save VM should restart if restart checkbox is checked Changes should show  In Form In YAML In VM list    </description>
    </item>
    
    <item>
      <title>Edit VM via form with CPU and Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-cpu-and-memory/</guid>
      <description> Edit VM Save  Expected Results  VM should save VM should restart if restart checkbox is checked Changes should show  In Form In YAML In VM list    </description>
    </item>
    
    <item>
      <title>Edit VM via form with Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-form-with-memory/</guid>
      <description> Edit VM Save  Expected Results  VM should save VM should restart if restart checkbox is checked Changes should show  In Form In YAML In VM list    </description>
    </item>
    
    <item>
      <title>Edit VM via YAML with CPU (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu/</guid>
      <description> Edit VM Save  Expected Results  VM should save VM should restart if restart checkbox is checked Changes should show  In Form In YAML In VM list    </description>
    </item>
    
    <item>
      <title>Edit VM via YAML with CPU and Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-cpu-and-memory/</guid>
      <description> Edit VM Save  Expected Results  VM should save VM should restart if restart checkbox is checked Changes should show  In Form In YAML In VM list    </description>
    </item>
    
    <item>
      <title>Edit VM via YAML with Memory (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-vm-via-yaml-with-memory/</guid>
      <description> Edit VM Save  Expected Results  VM should save VM should restart if restart checkbox is checked Changes should show  In Form In YAML In VM list    </description>
    </item>
    
    <item>
      <title>Edit VM with resources that are only on one node in cluster CPU and Memory</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/edit-a-vm-with-resources-that-are-only-on-one-node-in-cluster-cpu-and-memory/</guid>
      <description> Edit a VM with resources that are only available on one node in cluster.  Expected Results  VM should save VM should be reassigned to node that has available resources VM should boot VM should pass health checks  </description>
    </item>
    
    <item>
      <title>Edit VM YAML Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/q-negative-edit-vm-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/q-negative-edit-vm-yaml/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Edit the VM via YAML Save the VM  Expected Results  SSH to the node and check the nodes has components deleted.  </description>
    </item>
    
    <item>
      <title>Edit volume decrease size via YAML</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-decrase-size-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-decrase-size-yaml/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume as YAML Decrease size Click Save Connect to VM via console Check size of root disk  Expected Results  VM should stop VM should reboot after saving Disk should be resized  </description>
    </item>
    
    <item>
      <title>Edit volume decrease size via YAML</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-decrease-size-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-decrease-size-form/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume as YAML Decrease size Click Save Connect to VM via console Check size of root disk  Expected Results  VM should stop VM should reboot after saving Disk should be resized  </description>
    </item>
    
    <item>
      <title>Edit Volume Form add label</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-form-add-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-form-add-label/</guid>
      <description> Navigate to volumes page Edit Volume with Form Click Labels Add label Click Save Open VM again and click the config tab Verify that label was saved  Expected Results  Volume should save Label should add Label should show when re-opened  </description>
    </item>
    
    <item>
      <title>Edit volume increase size via form</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-form/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-form/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume via form Increase size Click Save Connect to VM via console Check size of root disk  Expected Results  VM should stop VM should reboot after saving Disk should be resized  </description>
    </item>
    
    <item>
      <title>Edit volume increase size via YAML (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-increase-size-yaml/</guid>
      <description> Stop the vm Navigate to volumes page Edit Volume as YAML Increase size Click Save Connect to VM via console Check size of root disk  Expected Results  VM should stop VM should reboot after saving Disk should be resized  </description>
    </item>
    
    <item>
      <title>Edit Volume YAML add label (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/edit-volume-yaml-add-label/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/edit-volume-yaml-add-label/</guid>
      <description> Navigate to volumes page Edit Volume as YAML Add label to config Click Save Open VM again and click the config tab Verify that label was saved  Expected Results  Volume should save Label should add Label should show when re-opened  </description>
    </item>
    
    <item>
      <title>Enabling vlan on a bonded NIC on vagrant install</title>
      <link>https://harvester.github.io/tests/manual/_incoming/enabling-vlan-on-bonded-nic-vagrant-install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/enabling-vlan-on-bonded-nic-vagrant-install/</guid>
      <description>Related issues: #1541 Enabling vlan on a bonded NIC breaks the Harvester setup  Category:  Network  Verification Steps  Pull ipxe example from https://github.com/harvester/ipxe-examples Vagrant pxe install 3 nodes harvester Access harvester settings page Open settings -&amp;gt; vlan Enable virtual network and set with bond0 Navigate to every page to check harvester is working Create a vlan based on bon0  Expected Results Enable virtual network with bond0 will not make harvester service out of work.</description>
    </item>
    
    <item>
      <title>Filter backups</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/filter-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/filter-backups/</guid>
      <description> Enter in string in filter input field  Columns available for matching:  State  &amp;ldquo;Ready&amp;rdquo; &amp;ldquo;Progressing&amp;rdquo;   Name Target VM   With string  With matching string  Input Clear   With non-matching string  Input Clear Clear String        Expected Results  List should filter based on string List should re-populate after clearing string  </description>
    </item>
    
    <item>
      <title>First Time Login</title>
      <link>https://harvester.github.io/tests/manual/authentication/first-time-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/first-time-login/</guid>
      <description> After successful installation of Harvester using Iso, on navigating to UI, user should be prompted to change the password. Verify the password rules  Expected Results  User should be able to login  </description>
    </item>
    
    <item>
      <title>Guest CSI Driver</title>
      <link>https://harvester.github.io/tests/manual/node-driver/guest-csi-driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/guest-csi-driver/</guid>
      <description>Start rancher using docker in a vm and start harvester in another Import harvester into rancher from &amp;ldquo;Virtualization Management&amp;rdquo; page On rancher, enable harvester node driver at &amp;ldquo;Cluster Management&amp;rdquo; -&amp;gt; &amp;ldquo;Drivers&amp;rdquo; -&amp;gt; &amp;ldquo;Node Driver&amp;rdquo; Go back to &amp;ldquo;Cluster Management&amp;rdquo; and create a rke2 cluster using Harvester Once the created cluster is active on the &amp;ldquo;Cluster Management&amp;rdquo; page, click on the &amp;ldquo;Explore&amp;rdquo; Go to &amp;ldquo;Workload&amp;rdquo; -&amp;gt; &amp;ldquo;Deployment&amp;rdquo; and &amp;ldquo;Create&amp;rdquo; a new deployment, during which in the page of &amp;ldquo;Storage&amp;rdquo;, click on &amp;ldquo;Add Volume&amp;rdquo; and select &amp;ldquo;Create Persistent Volume Claim&amp;rdquo; and select &amp;ldquo;Harvester&amp;rdquo; in the &amp;ldquo;Storage Class&amp;rdquo; Click &amp;ldquo;Create&amp;rdquo; to create the deployment Verify that on the Harvester side, a new volume is created.</description>
    </item>
    
    <item>
      <title>Host list should display the disk error message on failure</title>
      <link>https://harvester.github.io/tests/manual/_incoming/host-list-should-display-disk-error-message-on-failure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/host-list-should-display-disk-error-message-on-failure/</guid>
      <description> Related issue: #1167 Host list should display the disk error message on table  Category:  Storage  Verification Steps  Shutdown existing node vm machine Run &amp;ldquo;qemu-img create&amp;rdquo; command to make a nvme.img Edit quem/kvm xml setting to attach the nvme image Start VM Open hostpage and edit your target node config Add the new nvme disk Shutdown VM Remove the attach device setting in VＭ xml file Start VM Open Host page, the targe node will show warning with unready and unscheduable disk exists  Expected Results  If host encounter disk ready or schedule failure, on host page the &amp;ldquo;disk state&amp;rdquo; will show warning With a hover tip &amp;ldquo;Host have unready or unschedulable disks&amp;rdquo;   Can create load balancer correctly with health check setting  </description>
    </item>
    
    <item>
      <title>Http proxy setting on harvester</title>
      <link>https://harvester.github.io/tests/manual/_incoming/http-proxy-setting-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/http-proxy-setting-harvester/</guid>
      <description>Related issue: #1218 Missing http proxy settings on rke2 and rancher pod
  Related issue: #1012 Failed to create image when deployed in private network environment
  Category:  Network  Environment setup Setup an airgapped harvester
 Clone ipxe example repository https://github.com/harvester/ipxe-examples Edit the setting.xml file under vagrant ipxe example Set offline: true Use ipxe vagrant example to setup a 3 nodes cluster  Verification Steps  Open Settings, edit http-proxy with the following values  HTTP_PROXY=http://proxy-host:port HTTPS_PROXY=http://proxy-host:port NO_PROXY=localhost,127.</description>
    </item>
    
    <item>
      <title>Import and make changes to clusternetwork resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-clusternetwork/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-clusternetwork/</guid>
      <description>Import clusternetwork resource  terraformer import harvester -r clusternetwork  Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: default_physical_nic, enable in the clusternetwork.tf file Check the change through either the UI or the API  Expected Results  Import output  terraformer import harvester -r clusternetwork 2021/08/04 15:43:25 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to image resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-image/</guid>
      <description>Import image resource terraformer import harvester -r image Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: description, display_name, name, namespace and url in the image.tf file Check the change through either the UI or the API  Expected Results  Import output  terraformer import harvester -r image 2021/08/04 16:14:52 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to network resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-network/</guid>
      <description>Import network resource terraformer import harvester -r network Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: name, namespace and vlan_id in the network.tf file Check the change through either the UI or the API  Expected Results  Import output  terraformer import harvester -r network 2021/08/04 16:14:08 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to ssh_key resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-ssh-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-ssh-key/</guid>
      <description>Import ssh_key resource terraformer import harvester -r ssh_key Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: name, namespace and public_key in the ssh_key.tf file Check the change through either the UI or the API  Expected Results  Import output  terraformer import harvester -r ssh_key 2021/08/04 16:14:36 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to virtual machine resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-virtual-machine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-virtual-machine/</guid>
      <description>Import virtual machine resource terraformer import harvester -r virtualmachine Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: cpu, memory, name in the virtualmachine.tf file Check the change through either the UI or the API  Expected Results  Import output  terraformer import harvester -r virtualmachine 2021/08/04 16:15:08 harvester importing.</description>
    </item>
    
    <item>
      <title>Import and make changes to volume resource</title>
      <link>https://harvester.github.io/tests/manual/terraformer/import-edit-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraformer/import-edit-volume/</guid>
      <description>Import volume resource terraformer import harvester -r volume Replace the provider (already explained in the installation process above) terraform plan and apply command should print &amp;ldquo;No changes.&amp;rdquo; Alter the resource and check with terraform plan then terraform apply For instance, alter the following properties: name, namespace in the volume.tf file Check the change through either the UI or the API  Expected Results  Import output  terraformer import harvester -r volume 2021/08/04 16:15:29 harvester importing.</description>
    </item>
    
    <item>
      <title>Import External Harvester</title>
      <link>https://harvester.github.io/tests/manual/node-driver/import-external-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/import-external-harvester/</guid>
      <description>With Rancher &amp;lt; 2.6:
 Deploy the rancher and harvester clusters separately In the rancher, add a harvester node template Select &amp;ldquo;External Harvester&amp;rdquo;, and refer to &amp;ldquo;Test Data&amp;rdquo; for other value settings. Use this template to create the corresponding cluster With Rancher 2.6: Home page / Import Existing / Generic Add cluster name and click on Create Follow the registration steps  Expected Results  The status of the created cluster shows active The status of the corresponding vm on harvester active The information displayed on rancher and harvester matches the template configuration  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access External Harvester Host: Port: 443 Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Import internal harvester</title>
      <link>https://harvester.github.io/tests/manual/node-driver/import-internal-harvester/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/import-internal-harvester/</guid>
      <description>enable harvester&amp;rsquo;s rancher-enabled setting Click the rancher button in the upper right corner to access the internal rancher add a harvester node template Select &amp;ldquo;Internal Harvester&amp;rdquo;, and refer to &amp;ldquo;Test Data&amp;rdquo; for other value settings. Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active the status of the corresponding vm on harvester active the information displayed on rancher and harvester matches the template configuration  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Initiate multiple migrations at one time</title>
      <link>https://harvester.github.io/tests/manual/live-migration/initiate-multple-migrations-same-time/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/initiate-multple-migrations-same-time/</guid>
      <description> Initiate live migration for a vm. While the live migration is in progress, initiate another migration  Expected Results  Both migration should work fine. The VMs should be accessible after the migration  </description>
    </item>
    
    <item>
      <title>Install 2 node Harvester with a Harvester token with multiple words</title>
      <link>https://harvester.github.io/tests/manual/_incoming/812-multiple-word-harvester-token/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/812-multiple-word-harvester-token/</guid>
      <description> Related issues: #812 ISO install accepts multiple words for &amp;lsquo;cluster token&amp;rsquo; value resulting in failure to join cluster  Verification Steps  Start Harvester install from ISO At the &amp;lsquo;Cluster token&amp;rsquo; prompt, enter, here are words Proceed to complete the installation Boot a secondary host from the installation ISO and select the option to join an existing cluster At the &amp;lsquo;Cluster token&amp;rsquo; prompt, enter, here are words Proceed to complete the installation Verify both hosts show in hosts list at VIP  Expected Results  Install should complete successfully Host should add with no errors Both hosts should show up  </description>
    </item>
    
    <item>
      <title>Install Harvester from USB disk</title>
      <link>https://harvester.github.io/tests/manual/_incoming/install_via_usb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/install_via_usb/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1200
Verify Items  Harvester can be installed via USB stick  Case: Install Harvester via USB disk  Follow the instruction to create USB disk Harvester should able to be installed via the USB on UEFI-based bare metals  </description>
    </item>
    
    <item>
      <title>Install Harvester on a bare Metal node using ISO image</title>
      <link>https://harvester.github.io/tests/manual/deployment/install-bare-metal-iso/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/install-bare-metal-iso/</guid>
      <description>Install using ISO image https://docs.harvesterhci.io/v0.3/install/iso-install/
Expected Results  On completion of the installation, Harvester should provide the management url and show status. Harvester and Longhorn components should be up and running in the cluster. Verify the memory, cpu and storage size shown on the Harvester UI  </description>
    </item>
    
    <item>
      <title>Install Harvester on a bare Metal node using ISO image</title>
      <link>https://harvester.github.io/tests/manual/deployment/install-nested-virtualization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/install-nested-virtualization/</guid>
      <description>Install using ISO image https://docs.harvesterhci.io/v0.3/install/iso-install/
Expected Results  On completion of the installation, Harvester should provide the management url and show status. Harvester and Longhorn components should be up and running in the cluster. Verify the memory, cpu and storage size shown on the Harvester UI  </description>
    </item>
    
    <item>
      <title>Install Harvester on a bare Metal node using PXE boot (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/install-bare-metal-pxe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/install-bare-metal-pxe/</guid>
      <description>Install Harvester using PXE boot https://docs.harvesterhci.io/v0.3/install/pxe-boot-install/
Expected Results  On completion of the installation, Harvester should provide the management url and show status. Harvester and Longhorn components should be up and running in the cluster. Verify the memory, cpu and storage size shown on the Harvester UI  </description>
    </item>
    
    <item>
      <title>Install Harvester on NVMe SSD</title>
      <link>https://harvester.github.io/tests/manual/_incoming/install_on_nvme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/install_on_nvme/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1627
Verify Items  Harvester can detect NVMe SSD when installing Harvester can be installed on NVMe SSD  Case: Install Harvester on NVMe disk  Create block image as NVMe disk  Run dd if=/dev/zero of=/var/lib/libvirt/images/nvme145.img bs=1M count=148480 Then Change file owner chown qemu:qemu /var/lib/libvirt/images/nvme145.img   Create VM via virt-manager  Select Manual install, set Generic OS, Memory:9216, CPUs:8, Uncheck enable storage&amp;hellip; and check customize configuration before install Select Firmware to use UEFI x86_64 (use usr/share/qemu/ovmf-x86_64-code.</description>
    </item>
    
    <item>
      <title>Install Option `HwAddr` for Network Interface</title>
      <link>https://harvester.github.io/tests/manual/_incoming/hwaddr_configre_option/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/hwaddr_configre_option/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1064
Verify Items  Configure Option HwAddr is working on install configuration  Case: Use HwAddr to install harvester via PXE  Install Harvester with PXE installation, set hwAddr instead of name in install.networks Harvester should installed successfully  </description>
    </item>
    
    <item>
      <title>Install Option `install.device` support symbolic link</title>
      <link>https://harvester.github.io/tests/manual/_incoming/install_symblic_link/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/install_symblic_link/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1462
Verify Items  Disk&amp;rsquo;s symbolic link can be used in install configure option install.device  Case: Harvester install with configure symbolic link on install.device  Install Harvester with any nodes login to console, use ls -l /dev/disk/by-path to get disk&amp;rsquo;s link name Re-install Harvester with configure file, with set the disk&amp;rsquo;s link name instead. Harvester should be install successfully  </description>
    </item>
    
    <item>
      <title>Installation of the Harvester terraform provider (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/install-terraform-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/install-terraform-provider/</guid>
      <description>Follow the instruction of the README
Expected Results The provider is initialized and the terraform init command succeeds:
Initializing provider plugins... - Finding harvester/harvester versions matching &amp;quot;~&amp;gt; 0.1.0&amp;quot;... - Installing harvester/harvester v0.1.0... - Installed harvester/harvester v0.1.0 (unauthenticated) ... Terraform has been successfully initialized! </description>
    </item>
    
    <item>
      <title>keypairs.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/keypairs.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/keypairs.harvesterhci.io/</guid>
      <description>GUI  Enable VLAN network in settings Create a network with VLAN 5 and assume its name is my-network. C1. reate another network with VLAN 5: it should fails with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: VLAN ID 5 is already allocated Create a VM on VLAN 5, delete network my-network and it should fail with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: network my-network is still used by vm(s): vm-test in a modal.</description>
    </item>
    
    <item>
      <title>Login after password reset</title>
      <link>https://harvester.github.io/tests/manual/authentication/login-after-password-reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/login-after-password-reset/</guid>
      <description> Enter the wrong credential. Enter the correct credential  Expected Results  Login should fail. Login should pass  </description>
    </item>
    
    <item>
      <title>Logout from the UI and login again</title>
      <link>https://harvester.github.io/tests/manual/authentication/logout-then-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/logout-then-login/</guid>
      <description> Logout from the UI and Log in again  Expected Results  User should be able to logout/login successfully.  </description>
    </item>
    
    <item>
      <title>Maintenance mode for host with multiple VMs</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-multiple-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-multiple-vm/</guid>
      <description> Put host in maintenance mode Migrate VMs Wait for VMs to migrate Wait for any vms to migrate off Do health check on VMs  Expected Results  Host should start to go into maintenance mode Any VMs should migrate off Host should go into maintenance mode  </description>
    </item>
    
    <item>
      <title>Maintenance mode for host with one VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-one-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-one-vm/</guid>
      <description> Put host in maintenance mode Migrate VMs Wait for VMs to migrate Wait for any vms to migrate off Do health check on VMs  Expected Results  Host should start to go into maintenance mode Any VMs should migrate off Host should go into maintenance mode  </description>
    </item>
    
    <item>
      <title>Maintenance mode on node with no vms (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-no-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-no-vm/</guid>
      <description> Put host in maintenance mode Wait for host to go from entering maintenance mode to maintenance mode.  Expected Results  Host should start to go into maintenance mode Host should go into maintenance mode  </description>
    </item>
    
    <item>
      <title>Manual upgrade from 0.3.0 to 1.0.0</title>
      <link>https://harvester.github.io/tests/manual/_incoming/manual-upgrade-from-0.3.0-to-1.0.0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/manual-upgrade-from-0.3.0-to-1.0.0/</guid>
      <description>Related issues: #1644 Harvester pod crashes after upgrading from v0.3.0 to v1.0.0-rc1 (contain vm backup before upgrade)
  Related issues: #1588 VM backup cause harvester pod to crash
  Notice We recommend using zero downtime upgrade to upgrade harvester. Manual upgrade is for advance usage and purpose.
Category:  Manual Upgrade  Verification Steps  Download harvester v0.3.0 iso and do checksum Download harvester v1.0.0 iso and do checksum Use ISO Install a 4 nodes harvester cluster Create several OS images from URL Create ssh key Enable vlan network with harvester-mgmt Create virtual network vlan1 with id 1 Create 2 virtual machines   ubuntu-vm: 2 core, 4GB memory, 30GB disk   Setup backup target Take a backup from ubuntu vm Peform manual upgrade steps in the following docudment  upgrade process Follow the manual upgrade steps to upgrade from v0.</description>
    </item>
    
    <item>
      <title>Mark some features as experimental</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1671-mark-experimental-features/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1671-mark-experimental-features/</guid>
      <description> Related issues: #1671 Mark external Harvester cluster provisioning support as experimental  Verification Steps  Verify that external Harvester is marked as experiemental  Verify that Cloud Credentials is marked as experimental  Verify that external is marked as experimental in add node template   Expected Results  All external Harvester fields should be marked as experimental  </description>
    </item>
    
    <item>
      <title>Memory overcommit on VM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/memory_overcommit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/memory_overcommit/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1537
Verify Items  Overcommit can be edit on Dashboard VM can allocate exceed Memory on the host Node VM can chage allocated Memory after created  Case: Update Overcommit configuration  Install Harvester with any Node Login to Dashboard, then navigate to Advanced Settings Edit overcommit-config The field of Memory should be editable Created VM can allocate maximum Memory should be &amp;lt;HostMemory&amp;gt; * [&amp;lt;overcommit-Memory&amp;gt;/100] - &amp;lt;Host Reserved&amp;gt;  Case: VM can allocate Memory more than Host have  Install Harvester with any Node Create a cloud image for VM Creation Create a VM with &amp;lt;HostMemory&amp;gt; * 1.</description>
    </item>
    
    <item>
      <title>Migrate a turned on VM from one host to another</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-turned-on-vm-to-another-host/</guid>
      <description> Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate a VM created with cloud init config data</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-cloud-init/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-cloud-init/</guid>
      <description> Create a new VM with cloud init config data Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate a VM created with user data config</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-user-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-user-data/</guid>
      <description> Create a new VM with a password specified by user data config Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate a VM that has multiple volumes</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-volumes/</guid>
      <description> Create a new VM with a root disk and a CDROM volume Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate a VM that was created from a template</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-template/</guid>
      <description> Create a new VM from a template Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate a VM that was created using a restore backup to new VM</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-restore-to-new/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-created-from-restore-to-new/</guid>
      <description> Take an existing backup Restore the backup to a new VM Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate a VM with 1 backup</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-one-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-one-backup/</guid>
      <description> Create a new VM Create a backup Add a new file to the home directory Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate a VM with a saved SSH Key</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-ssh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-with-ssh/</guid>
      <description> Create a new VM with an SSH key Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate a VM with multiple backups</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-backups/</guid>
      <description> Create a new VM Create a backup Add a new file to the home directory Create a new backup Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate a VM with multiple networks</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-networks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-vm-multiple-networks/</guid>
      <description> Create a new VM with  one management network in masquerade mode one VLAN network   Create a new file on the machine Migrate the VM from one host in the cluster to another Connect via console Check for the file Change the file and save it Verify that you can close and open the file again  Expected Results  File should create correctly VM should go into migrating status VM should go out of migrating status It should show the new node on the host column in the VM list It should have the same IP You should be able to edit and re-open the file  </description>
    </item>
    
    <item>
      <title>Migrate back VMs that were on host after taking host out of maintenance mode</title>
      <link>https://harvester.github.io/tests/manual/hosts/q-maintenance-mode-migrate-back-vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/q-maintenance-mode-migrate-back-vms/</guid>
      <description>Migrate all VMs back to host that were migrated off  Expected Results I&amp;rsquo;m not sure about the expected behavior on this. I&amp;rsquo;m checking.</description>
    </item>
    
    <item>
      <title>Migrate to Node without replicaset</title>
      <link>https://harvester.github.io/tests/manual/live-migration/migrate-to-node-without-replicaset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/migrate-to-node-without-replicaset/</guid>
      <description> Create a new VM on a 4 node cluster Check which nodes have copies of the replica set Migrate the VM to the host that does not have the volume  Expected Results  VM should create correctly  </description>
    </item>
    
    <item>
      <title>Migrate VM from Restored backup</title>
      <link>https://harvester.github.io/tests/manual/_incoming/restored_vm_migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/restored_vm_migration/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1086
Verify Items  VM can be migrate to any node with any times  Case: Migrate a restored VM  Install Harvester with at least 2 nodes setup backup-target with NFS Create image for VM creation Create VM a Add file with some data in VM a Backup VM a as a-bak Restore backup a-bak into VM b Start VM b then check added file should exist with same content Migrate VM b to another node, then check added file should exist with same content Migrate VM b again, then check added file should exist with same content  </description>
    </item>
    
    <item>
      <title>Move Longhorn storage to another partition</title>
      <link>https://harvester.github.io/tests/manual/_incoming/move-longhorn-storage-to-another-partition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/move-longhorn-storage-to-another-partition/</guid>
      <description>Related issue: #1316 Move Longhorn storage to another partition  Category:  Storage  Test Scenarios  Case 1: UEFI + GPT (Disk &amp;lt; MBR Limit) Case 2: BIOS + No MBR (Disk &amp;lt; MBR Limit) Case 3: BIOS + Force MBR (Disk &amp;lt; MBR Limit) Case 4: BIOS + No MBR (Disk &amp;gt; MBR Limit) Case 5: BIOS + Force MBR (Disk &amp;gt; MBR Limit) Case 6: UEFI + GPT (Disk &amp;gt; MBR Limit)  Environment setup   Test Environment: 1 node harvester on local kvm machine</description>
    </item>
    
    <item>
      <title>Multi-browser login</title>
      <link>https://harvester.github.io/tests/manual/authentication/multi-browser-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/multi-browser-login/</guid>
      <description> Login via Chrome, firefox, edge, safari etc  Expected Results  Chrome, firefox, edge, safari etc should have same behavior.  </description>
    </item>
    
    <item>
      <title>Negative create backup on store that is full (NFS)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-full-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-full-backup-target/</guid>
      <description> Initiate a backup with existing VM where the NFS store is full  Expected Results  You should get an error  </description>
    </item>
    
    <item>
      <title>Negative Create Backup Target</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-create-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-create-backup-target/</guid>
      <description> Open up Backup-target in settings Input Incorrect server info Save  Expected Results  You should get an error on saving  </description>
    </item>
    
    <item>
      <title>Negative delete backup while restore is in progress</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-backup-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-backup-while-restoring/</guid>
      <description> Create a backup of VM which has data more than 10Gi. Add 2Gi data in the same VM. Initiate deletion of the backup. While deletion is in progress, create another backup  Expected Results  Creation of backup should be prevented as there is a deletion is in progress. Once the deletion is completed, the backup creation should take place  </description>
    </item>
    
    <item>
      <title>Negative delete multiple backups</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-multiple-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-multiple-backups/</guid>
      <description> Disconnect Backup Target Select multiple Backups from Backups list Click Delete  Expected Results  You should get an error  </description>
    </item>
    
    <item>
      <title>Negative delete single backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-single-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-delete-single-backup/</guid>
      <description> Take down backup target either by account, or via network blocking Delete backup from backups list  Expected Results  You should get an error  </description>
    </item>
    
    <item>
      <title>Negative delete Volume that is in use (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/negative-delete-volume-that-is-in-use/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/negative-delete-volume-that-is-in-use/</guid>
      <description> Navigate to Volumes page and check for a volume in use by a VM Try to delete volume Click delete on modal  Expected Results  Page should load You should get an error message on the delete modal  </description>
    </item>
    
    <item>
      <title>Negative disrupt backup server while restore is in progress</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-disrupt-backup-target-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-disrupt-backup-target-while-restoring/</guid>
      <description> Initiate a backup restore from NFS server. Disconnect network from NFS server for 5 secs Verify the restore status  Expected Results  The restore is not be interrupted and should complete. Data should be intact  </description>
    </item>
    
    <item>
      <title>Negative edit backup read from file YAML</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-file/</guid>
      <description> Disconnect backup target Edit YAML for backup Read from File Show Diff Save  Expected Results  You should get an error on saving  </description>
    </item>
    
    <item>
      <title>Negative edit backup YAML</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-yaml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-edit-backup-yaml/</guid>
      <description> Disconnect backup target Edit YAML for backup Show Diff Save  Expected Results  You should get an error on saving  </description>
    </item>
    
    <item>
      <title>Negative initiate a backup while system is taking another backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-while-taking-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-backup-while-taking-backup/</guid>
      <description> Start a VM backup, bk-1 of a VM which has data d1 While the backup is in progress, write some more data d2 in the VM disk and initiate another backup bk-2. Verify the backup 1 and backup 2  Expected Results  Backup bk-1 should have only d1 data backup bk-2 should have data d1 and d2  </description>
    </item>
    
    <item>
      <title>Negative migrate a turned on VM from one host to another</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-migrate-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-migrate-vm/</guid>
      <description> Migrate the VM from one host in the cluster to another Turn off/disconnect node while migrating  Expected Results  Migration should fail You should get an error message in the status  </description>
    </item>
    
    <item>
      <title>Negative network comes back up after reboot external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/negative-vlan-after-reboot/</guid>
      <description> Start pinging the VM reboot the VM  Expected Results  The VM should respond The VM should reboot The pings should stop getting responses The pings should start getting responses again  </description>
    </item>
    
    <item>
      <title>Negative network comes back up after reboot management network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-management-after-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/negative-management-after-reboot/</guid>
      <description> Start pinging the VM from the management network reboot the VM  Expected Results  The VM should respond The VM should reboot The pings should stop getting responses The pings should start getting responses again  </description>
    </item>
    
    <item>
      <title>Negative network disconnection for a longer time while migration is in progress</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-network-disconnect-while-migrating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-network-disconnect-while-migrating/</guid>
      <description> Initiate VM migration While migration is in progress, disconnect network for 100 sec on the node where the VM is scheduled  Expected Results  Migration should fail but volume data should be intact The VM should be accessible during the migration and should also be accessible once the migration fails  </description>
    </item>
    
    <item>
      <title>Negative network disconnection for a short time while migration is in progress</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-network-disruption-while-migrating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-network-disruption-while-migrating/</guid>
      <description> Initiate VM migration. While migration is in progress, disconnect network for 5 sec on the node where the VM is scheduled  Expected Results  Migration should resume once the network is up again The VM should be accessible during and after the migration  </description>
    </item>
    
    <item>
      <title>Negative node down while migration is in progress</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-node-down-while-migrating/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-node-down-while-migrating/</guid>
      <description> Initiate VM migration. While migration is in progress, shut the node where the VM is scheduled. After failure, initiate the migration to another node  Expected Results  Migration should fail but volume data should be intact The VM should be accessible on older node The migration scheduled for another node should work fine The VM should be accessible during and after the migration  </description>
    </item>
    
    <item>
      <title>Negative node un-schedulable during live migration</title>
      <link>https://harvester.github.io/tests/manual/live-migration/negative-node-unschedulable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/negative-node-unschedulable/</guid>
      <description>Prerequisite:  Cluster is of 3 nodes. VM is running on Node-1 Node-2 and Node-3 don&amp;rsquo;t have space to migrate a VM to them.  Steps:  Create a vm on node-1 Migrate the VM.  Expected Results  Migration should not be started. Relevant error should be shown on the GUI. The existing VM should be accessible and the health check of the VM should be fine  </description>
    </item>
    
    <item>
      <title>Negative Power down the node where the VM is getting replaced by the restore</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring-replace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring-replace/</guid>
      <description> Initiate a restore with existing VM. While the restore is in progress and VM is starting on a node, shut down the node  Expected Results  You should get an error  </description>
    </item>
    
    <item>
      <title>Negative power down the node where the VM is getting restored</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-power-down-node-while-restoring/</guid>
      <description> Initiate a restore. While the restore is in progress and VM is starting on a node, shut down the node  Expected Results  The restore should fail  </description>
    </item>
    
    <item>
      <title>Negative restore backup replace existing VM</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace/</guid>
      <description> On multi-node setup bring down node that is hosting VM Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore  Expected Results  You should get an error on restoring  </description>
    </item>
    
    <item>
      <title>Negative restore backup replace existing VM with backup from same VM that is turned on</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-deleting-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-deleting-backup/</guid>
      <description> Make sure VM is turned on Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Delete backup while restoring  Expected Results  You should get an error  </description>
    </item>
    
    <item>
      <title>Negative restore backup replace existing VM with backup from same VM that is turned on (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-turned-on/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/negative-restore-backup-replace-while-turned-on/</guid>
      <description> Make sure VM is turned on Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore  Expected Results  You get an error that you have to stop VM before restoring backup  </description>
    </item>
    
    <item>
      <title>network-attachment-definitions.k8s.cni.cncf.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/q-network-attachment-definitions.k8s.cni.cncf.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/q-network-attachment-definitions.k8s.cni.cncf.io/</guid>
      <description>GUI  Enable VLAN network in settings Create a network with VLAN 5 and assume its name is my-network. Create another network with VLAN 5: it should fails with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: VLAN ID 5 is already allocated Create a VM on VLAN 5, delete network my-network and it should fail with: admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: network my-network is still used by vm(s): vm-test in a modal  Expected Results GUI Unsure of desired behavior.</description>
    </item>
    
    <item>
      <title>Node Labeling for VM scheduling</title>
      <link>https://harvester.github.io/tests/manual/_incoming/node_labeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/node_labeling/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1416
Verify Items  Host labels can be assigned during installation via config-create / config-join YAML. Host labels can be managed post installation via the Harvester UI. Host label information can be accessed in Rancher Virtualization Management UI.  Case: Label node when installing  Install Harvester with config file and os.labels option Navigate to Host details then navigate to Labels in Config Check additional labels should be displayed  Case: Label node after installed  Install Harvester with at least 2 nodes Navigate to Host details then navigate to Labels in Config Use edit config to modify labels Reboot the Node and wait until its state become active Navigate to Host details then Navigate to Labels in Config Check modified labels should be displayed  Case: Node&amp;rsquo;s Label availability  Install Harvester with at least 2 nodes Navigate to Host details then navigate to Labels in Config Use edit config to modify labels Reboot the Node and wait until its state become active Navigate to Host details then Navigate to Labels in Config Check modified labels should be displayed Install Rancher with any nodes Navigate to Virtualization Management and import former created Harvester Wait Until state become Active Click Name field to visit dashboard repeat step 2-7, and both compare from Harvester&amp;rsquo;s dashboard (accessing via Harvester&amp;rsquo;s VIP)  </description>
    </item>
    
    <item>
      <title>Nodes with cordoned status should not be in VM migration list</title>
      <link>https://harvester.github.io/tests/manual/_incoming/nodes-with-cordoned-status-should-not-be-in-vm-migration-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/nodes-with-cordoned-status-should-not-be-in-vm-migration-list/</guid>
      <description>Related issues: #1501 Nodes with cordoned status should not be in the selection list for VM migration  Category:  Host  Verification Steps  Create multiple VMs on two of the nodes Set the idle node to cordoned state Edit any config of VM, click migrate Check the available node in the migration list  Expected Results Node set in cordoned state will not show up in the available migration list</description>
    </item>
    
    <item>
      <title>Power down a node out of three nodes available for the Cluster</title>
      <link>https://harvester.github.io/tests/manual/deployment/negative-power-off-one-node-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/negative-power-off-one-node-cluster/</guid>
      <description> Create a three nodes cluster for Harvester. Power down an added node.  Expected Results  On power down the node, the status of the node should become down. Harvester system system should be still up.  </description>
    </item>
    
    <item>
      <title>Power down and power up the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-power-down-power-up-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-power-down-power-up-node/</guid>
      <description>Create two vms on a cluster. Power down the node. Try to migrate a VM from the down node to active node. Leave the 2nd vm as it is. Power on the node  Expected Results  The 1st VM should be migrated to other node on manually doing it. The 2nd VM should be accessible once the node is up.  Known bugs https://github.com/harvester/harvester/issues/982</description>
    </item>
    
    <item>
      <title>Power down the management node.</title>
      <link>https://harvester.github.io/tests/manual/deployment/negative-power-down-management-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/negative-power-down-management-node/</guid>
      <description> Create a three nodes cluster for Harvester. Power down the first node which was added to the cluster.  Expected Results  On power down the node, the status of the node should become down. Harvester system system should be still up.  </description>
    </item>
    
    <item>
      <title>Power down the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-power-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-power-down-node/</guid>
      <description> Create two vms on a cluster. Power down the node. Try to migrate a VM from the down node to active node. Leave the 2nd vm as it is.  Expected Results  The 1st VM should be migrated to other node on manually doing it. The 2nd VM should be recovered from the lost node  </description>
    </item>
    
    <item>
      <title>Provision RKE2 cluster with resource quota configured</title>
      <link>https://harvester.github.io/tests/manual/_incoming/provision-rke2-cluster-with-resource-quota-configured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/provision-rke2-cluster-with-resource-quota-configured/</guid>
      <description>Related issues: #1455 Node driver provisioning fails when resource quota configured in project
  Related issues: #1449 Incorrect naming of project resource configuration
  Category:  Rancher Integration  Environment setup  Install the latest rancher from docker command  $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Test Scenarios   Scenario 1:
 Project with resource quota:  CPU Limit / CPU Reservation: 6000 / 6144 Memory Limit / Memory Reservation: 6000 / 6144      Scenario 2:</description>
    </item>
    
    <item>
      <title>PXE instll without iso_url field</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1439-pxe-install-without-iso-url-field/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1439-pxe-install-without-iso-url-field/</guid>
      <description> Related issues: #1439 PXE boot installation doesn&amp;rsquo;t give an error if iso_url field is missing  Environment setup This is easiest to test with the vagrant setup at https://github.com/harvester/ipxe-examples/tree/main/vagrant-pxe-harvester
 edit https://github.com/harvester/ipxe-examples/blob/main/vagrant-pxe-harvester/ansible/roles/harvester/templates/config-create.yaml.j2#L27 to be blank  Verification Steps  Run the vagrant ./setup.sh from the vagrant repo  Expected Results  You should get an error in the console for the VM when installing  </description>
    </item>
    
    <item>
      <title>Rancher import harvester enhancement</title>
      <link>https://harvester.github.io/tests/manual/_incoming/rancher-import-harvester-enhacement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/rancher-import-harvester-enhacement/</guid>
      <description>Related issues: #1330 Http proxy setting download image  Category:  Rancher Integration  Environment setup  Install the latest rancher from docker command  $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps  Installed a 3 nodes harvester cluster Import harvester to rancher in virtualization management Enable node driver and create cloud credential Provision a RKE2 cluster in rancher Confirm RKE2 cluster is fully operated, can explore it  Shutdown all 3 nodes server machine  Wait for 10 minutes Power on all harvester nodes server machines Confirm harvester is fully operated Confirm RKE2 vm is back to running  Check the RKE2 cluster status in rancher  Expected Results The RKE2 cluster in rancher should turn back to Running with no error after harvester server node machine is fully power off and power on.</description>
    </item>
    
    <item>
      <title>Rancher Resource quota management</title>
      <link>https://harvester.github.io/tests/manual/_incoming/resource_quota/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/resource_quota/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1450
Verify Items  Project&amp;rsquo;s Resource quotas can be updated correctly Namespace Default Limit should be assigned as the Project configured Namespace moving between projects should work correctly  Case: Create Namespace with Resource quotas  Install Harvester with any nodes Install Rancher Login to Rancher, import Harvester from Virtualization Management Access Harvester dashboard via Virtualization Management Navigate to Project/Namespaces, Create Project A with Resource quotas Create Namespace N1 based on Project A The Default value of Resource Quotas should be the same as Namespace Default Limit assigned in Project A Modifying resource limit should work correctly (when increasing/decreasing, the value should increased/decreased) After N1 Created, Click Edit Config on N1 resource limit should be the same as we assigned Increase/decrease resource limit then Save Click Edit Config on N1, resource limit should be the same as we assigned Click Edit Config on N1, then increase resource limit exceeds Project A&amp;rsquo;s Limit Click Save Button, error message should shown.</description>
    </item>
    
    <item>
      <title>Reboot a cluster and check VIP</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1669-reboot-cluster-check-vip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1669-reboot-cluster-check-vip/</guid>
      <description> Related issues: #1669 Unable to access harvester VIP nor node IP after reboot or fully power cycle node machines (Intermittent)  Verification Steps  Enable VLAN with NIC harvester-mgmt Create VLAN 1 Disable VLAN Enable VLAN again shutdown node 3, 2, 1 server machine Wait for 15 minutes Power on node 1 server machine, wait for 20 seconds Power on node 2 server machine, wait for 20 seconds Power on node 3 server machine Check if you can access VIP and each node IP  Expected Results  VIP should load the page and show on every node in the terminal  </description>
    </item>
    
    <item>
      <title>Reboot host that is in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-reboot-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-reboot-host/</guid>
      <description>For Host that is in maintenance mode and turned on Reboot host  Expected Results  Host should reboot Maintenance mode label in hosts list should go from yellow to red to yellow  Known Bugs https://github.com/harvester/harvester/issues/1272</description>
    </item>
    
    <item>
      <title>Reboot node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-reboot-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-reboot-node/</guid>
      <description> Create a vm on the cluster. Reboot the node where the vm exists. Reboot the node where there is no vm  Expected Results  On rebooting the node, once the node is back up and Harvester is started, the host should become available on the cluster.  </description>
    </item>
    
    <item>
      <title>Reboot the management node/added node.</title>
      <link>https://harvester.github.io/tests/manual/deployment/negative-reboot-management-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/negative-reboot-management-node/</guid>
      <description> Create a three nodes cluster for Harvester. Reboot the management node/added node.  Expected Results  Once the node is up after reboot, the node should become available in the cluster.  </description>
    </item>
    
    <item>
      <title>Recover cordon and maintenace node after harvester node machine reboot</title>
      <link>https://harvester.github.io/tests/manual/_incoming/recover-cordon-or-maintenace-node-after-harvester-node-machine-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/recover-cordon-or-maintenace-node-after-harvester-node-machine-reboot/</guid>
      <description>Related issues: #1493 When hosts are stuck in maintenance mode and the cluster is unstable you can&amp;rsquo;t access the UI  Category:  Host  Verification Steps  Create 3 virtual machine on 3 harvester nodes Cordon 1st and 2nd node,  Enable maintenance mode on 1st and 2nd node  We can&amp;rsquo;t cordon and enable maintenance node on the remaining node  Reboot 1st and 2nd node bare machine Wait for harvester machine back to service Login dashboard Disable maintenance mode on 1st and 2nd node  Expected Results  Cordon node and enter maintenance mode, after machine reboot, user can login harvester dashboard.</description>
    </item>
    
    <item>
      <title>Remove a management node from a 3 nodes cluster and add it back to the cluster by reinstalling it</title>
      <link>https://harvester.github.io/tests/manual/hosts/remove-management-node-then-reinstall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/remove-management-node-then-reinstall/</guid>
      <description> From a HA cluster with 3 nodes Delete one of the nodes after the node promotion(all 3 nodes are management nodes) Reinstall the removed node with the same node name and IP The rejoined node will be promoted to master automatically  Expected Results  The removed node should be able to rejoin the cluster without issues  Comments  Purpose is to cover this scenario: https://github.com/harvester/harvester/issues/1040 Check the job promotion with the command kubectl get jobs -n harvester-system If a node is stuck in the removing status, you likely face to this issue, execute this command as workaround: kubectl get node -o name &amp;lt;nodename&amp;gt; | xargs -i kubectl patch {} -p &#39;{&amp;quot;metadata&amp;quot;:{&amp;quot;finalizers&amp;quot;:[]}}&#39; --type=merge  </description>
    </item>
    
    <item>
      <title>Remove a node from the existing cluster</title>
      <link>https://harvester.github.io/tests/manual/deployment/remove-node-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/remove-node-cluster/</guid>
      <description>Remove node from the Harvester cluster using the Harvester UI  Expected Results The components of Harvester should get cleaned up from the node.</description>
    </item>
    
    <item>
      <title>Remove unavailable node with VMs on it</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-remove-unavailable-node-with-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-remove-unavailable-node-with-vm/</guid>
      <description>Create VMs on a host. Turn off Host Remove Host from hosts list  Expected Results  VMs should migrate to new host  Known Bugs https://github.com/harvester/harvester/issues/983</description>
    </item>
    
    <item>
      <title>Restore backup create new vm (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-create-new-vm/</guid>
      <description> Create a new file before restoring the backup and add some data Stop the VM where the backup was taken Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Validate that new file is no longer present on machine  Expected Results  Backup should restore VM should update to previous backup File should no longer be present  </description>
    </item>
    
    <item>
      <title>Restore Backup for VM that was live migrated (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-for-vm-live-migrated/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-for-vm-live-migrated/</guid>
      <description> Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Validate that new file is no longer present on machine  Expected Results  Backup should restore VM should update to previous backup File should no longer be present  </description>
    </item>
    
    <item>
      <title>Restore backup replace existing VM with backup from same VM (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-replace-existing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-backup-replace-existing/</guid>
      <description> Create a new file before restoring the backup and add some data Stop the VM Navigate to backups list Click restore Backup Select appropriate option Select backup Click restore Validate that new file is no longer present on machine  Expected Results  Backup should restore VM should update to previous backup File should no longer be present  </description>
    </item>
    
    <item>
      <title>Restore First backup in chained backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-first-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-first-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Validate that files didn&amp;rsquo;t change Restore to backup 1 Validate that  md5sum -c file1.</description>
    </item>
    
    <item>
      <title>Restore last backup in chained backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-last-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-last-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Validate that files didn&amp;rsquo;t change Restore to backup 3 Validate that  md5sum -c file1-2.</description>
    </item>
    
    <item>
      <title>Restore middle backup in chained backup</title>
      <link>https://harvester.github.io/tests/manual/backup-and-restore/restore-middle-backup-chained-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/backup-and-restore/restore-middle-backup-chained-backup/</guid>
      <description>Create a new VM Create a file named 1 and add some data using command dd if=/dev/urandom of=file1.txt count=100 bs=1M Compute md5sum : md5sum-1 Create a backup Overwrite file 1 Create file 2 Compute md5sum for file 1 and file 2 : md5sum-2, md5sum-3 Create Backup Overwrite the file 2 Create file 3 and compute md5sum for file 2 and file 3 : md5sum-4, md5sum-5 Create backup Validate that files didn&amp;rsquo;t change Restore to backup 2 Validate that  md5sum -c file1-2.</description>
    </item>
    
    <item>
      <title>Run multiple instances of the console</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/run-multiple-instances-console/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/run-multiple-instances-console/</guid>
      <description> Open up the console on two browsers to simulate multiple connections Login with both browsers create a new file on both instances Edit the file from the other instance and save Verify that you can see the changes from the other instance  Expected Results  You should be able to login from multiple browsers File should create File should update You should be able to see changes from all instances  </description>
    </item>
    
    <item>
      <title>Set backup target S3</title>
      <link>https://harvester.github.io/tests/manual/advanced/set-s3-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/set-s3-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose S3 Set valid S3 target Save  Expected Results  login should complete Settings should save You should not get an error message  </description>
    </item>
    
    <item>
      <title>Set backup-target NFS</title>
      <link>https://harvester.github.io/tests/manual/advanced/set-nfs-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/set-nfs-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose NFS Set valid NFS target Save  Expected Results  login should complete Settings should save You should not get an error message  </description>
    </item>
    
    <item>
      <title>Set backup-target NFS invalid target</title>
      <link>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-nfs-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-nfs-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose NFS Set invalid NFS target Save  Expected Results  login should complete Settings should save You should get an error message  </description>
    </item>
    
    <item>
      <title>Set backup-target S3 invalid target</title>
      <link>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-s3-backup-target/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/advanced/negative-set-invalid-s3-backup-target/</guid>
      <description> Log in as admin Navigate to advanced settings Edit config on backup-target Choose S3 Set invalid S3 target Save  Expected Results  login should complete Settings should save You should get an error message  </description>
    </item>
    
    <item>
      <title>Set maintenance mode on the last available node shouldn&#39;t be allowed</title>
      <link>https://harvester.github.io/tests/manual/_incoming/set-maintenance-mode-on-the-last-available-node-shouldnt-be-allowed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/set-maintenance-mode-on-the-last-available-node-shouldnt-be-allowed/</guid>
      <description>Related issues: #1014 Trying to set maintenance mode on the last available node shouldn&amp;rsquo;t be allowed  Category:  Host  Verification Steps   Create 3 vms located on node2 and node3   Open host page
  Set node 3 into maintenance mode
  Wait for virtual machine migrate to node 2
  Set node 2 into maintenance mode
  wait for virtual machine migrate to node 1</description>
    </item>
    
    <item>
      <title>Shut down host in maintenance mode and verify label change</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1272-shutdown-host-in-maintenance-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1272-shutdown-host-in-maintenance-mode/</guid>
      <description> Related issues: #1272 Shut down a node with maintenance mode should show red label  Verification Steps  Open host page Set a node to maintenance mode Turn off host vm of the node Check node status Turn on host Check node status  Expected Results  The node should go into maintenance mode The node label should go red When turned on the node status should go back to yellow  </description>
    </item>
    
    <item>
      <title>SSL Certificate</title>
      <link>https://harvester.github.io/tests/manual/_incoming/ssl-certificate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/ssl-certificate/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/761
Verify Items  generated kubeconfig is able to access kubenetes API new node able to join the cluster using the configured Domain Name create node with ssl-certificates settings is working as expected.  Case: Kubeconfig  Install Harvester with at least 2 nodes Generate self-signed TLS certificates from https://www.selfsignedcertificate.com/ with specific name Navigate to advanced settings, edit ssl-certificates settings Update generated .cert file to CA and Public Certificate, .</description>
    </item>
    
    <item>
      <title>Start Host in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-start-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-start-host/</guid>
      <description>For Host that is in maintenance mode and turned off Start host  Expected Results  Host should turn on Maintenance mode label in hosts list should go from red to yellow  Known bugs https://github.com/harvester/harvester/issues/1272</description>
    </item>
    
    <item>
      <title>Start VM and stop node Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm-and-stop-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm-and-stop-node/</guid>
      <description> Start the VM In a multi-node setup disconnect/shutdown the node where the VM is running  Expected Results  You should not be able to start the VM  </description>
    </item>
    
    <item>
      <title>Start VM Negative (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-start-vm/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Start the VM  Expected Results  You should not be able to start the VM  </description>
    </item>
    
    <item>
      <title>Stop VM Negative</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/negative-stop-vm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/negative-stop-vm/</guid>
      <description> In a multi-node setup disconnect/shutdown the node where the VM is running Stop the VM  Expected Results  The VM list should quickly update to not running, or some other error state  </description>
    </item>
    
    <item>
      <title>Support volume hot plug live migrate</title>
      <link>https://harvester.github.io/tests/manual/_incoming/support-volume-hot-unplug-live-migrate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/support-volume-hot-unplug-live-migrate/</guid>
      <description> Related issues: #1401 Support volume hot-unplug  Category:  Storage  Environment setup Setup an airgapped harvester
 Create an 3 nodes harvester cluster with large size disks  Verification Steps Scenario2: Live migrate VM not have hot-plugged volume before, do hot-plugged the unplugged.  Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click Detach volume Add volume again Migrate VM from one node to another Detach volume Add unplugged volume again  Expected Results  Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM  </description>
    </item>
    
    <item>
      <title>Support Volume Hot Unplug</title>
      <link>https://harvester.github.io/tests/manual/_incoming/support-volume-hot-unplug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/support-volume-hot-unplug/</guid>
      <description> Related issues: #1401 Support volume hot-unplug  Category:  Storage  Environment setup Setup an airgapped harvester
 Create an 3 nodes harvester cluster with large size disks  Scenario1: Live migrate VM already have hot-plugged volume to new node, then detach (hot-unplug) it Verification Steps  Create a virtual machine Create several volumes (without image) Add volume, hot-plug volume to virtual machine Open virtual machine, find hot-plugged volume Click de-attach volume Add volume again  Expected Results  Can hot-plug volume without error Can hot-unplug the pluggable volumes without restarting VM The de-attached volume can also be hot-plug and mount back to VM  </description>
    </item>
    
    <item>
      <title>Switch the vlan interface of harvester node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/switch-the-vlan-interface-of-harvester-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/switch-the-vlan-interface-of-harvester-node/</guid>
      <description>Related issues: #1464 VM pods turn to the terminating state after switching the VLAN interface  Category:  Network  Verification Steps  User ipxe-example to build up 3 nodes harvester Login harvester dashboard -&amp;gt; Access Settings Enable vlan network with harvester-mgmt NIC interface Create a VM using harvester-mgmt Disable vlan network Enable vlan network and select bond0 interface  Check host and vm is working Directly switch network interface from bond0 to harvester-mgmt without disable it.</description>
    </item>
    
    <item>
      <title>Take host out of maintenance mode that has been rebooted	(e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-rebooted/</guid>
      <description> For host in maintenance mode that has been rebooted take host out of maintenance mode  Expected Results  Host should go to Active Label shbould go green  </description>
    </item>
    
    <item>
      <title>Take host out of maintenance mode that has not been rebooted (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-not-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-enable-host-not-rebooted/</guid>
      <description> For host in maintenance mode that has not been rebooted take host out of maintenance mode  Expected Results  Host should go to Active Label shbould go green  </description>
    </item>
    
    <item>
      <title>Target Harvester by setting the variable kubeconfig with your kubeconfig file in the provider.tf file (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-variasble/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-variasble/</guid>
      <description>Define the kubeconfig variable in the provider.tf file  terraform { required_providers { harvester = { source = &amp;quot;registry.terraform.io/harvester/harvester&amp;quot; version = &amp;quot;~&amp;gt; 0.1.0&amp;quot; } } } provider &amp;quot;harvester&amp;quot; { kubeconfig = &amp;quot;/path/of/my/kubeconfig&amp;quot; }  Check if you can interact with the Harvester by creating resource like a SSH key Execute the terraform apply command  Expected Results  The resource should be created Apply complete! Resources: 1 added, 0 changed, 0 destroyed.</description>
    </item>
    
    <item>
      <title>Target Harvester with the default kubeconfig located in $HOME/.kube/config (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-kubeconfig-home/</guid>
      <description> Make sure the kubeconfig is defined in the file $HOME/.kube/config Check if you can interact with the Harvester by creating resource like a SSH key Execute the terraform apply command  Expected Results  The resource should be created Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Check if you can see your resource in the Harvester WebUI  </description>
    </item>
    
    <item>
      <title>Temporary network disruption</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-network-disruption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-network-disruption/</guid>
      <description> Create a vms on the cluster. Disable network of a node for sometime. e.g. 5 sec, 5 mins  Expected Results  VM should be accessible after the network is up.  </description>
    </item>
    
    <item>
      <title>Test a deployment with ALL resources at the same time (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/deployment-all-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/deployment-all-resources/</guid>
      <description>Re-use the previous generated TF files and group them all either in one directory or in the same file Generates a speculative execution plan with terraform plan command Create the resources with terraform apply command Check that all resources are correctly created/running on the Harvester cluster Destroy the resources with the command terraform destroy  Expected Results Refer to the harvester_ssh_key resource expected results</description>
    </item>
    
    <item>
      <title>Test aborting live migration</title>
      <link>https://harvester.github.io/tests/manual/live-migration/abort-live-migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/abort-live-migration/</guid>
      <description> On a VM that is turned on select migrate Start the migration Abort the migration  Expected Results  You should see the status move to migrating You should see the status move to aborting migration You should see the status move to running The VM should pass health checks  </description>
    </item>
    
    <item>
      <title>Test NTP server timesync</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1535-test-ntp-timesync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1535-test-ntp-timesync/</guid>
      <description> Related issues: #1535 NTP daemon in host OS  Environment setup This should be on at least a 3 node setup that has been running for several hours that had NTP servers setup during install
Verification Steps  SSH into nodes and verify times are close Verify NTP is active with sudo timedatectl status  Expected Results  Times should be within a minute of each other NTP should show as active  </description>
    </item>
    
    <item>
      <title>Test the harvester_clusternetwork resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-clusternetwork-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-clusternetwork-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test the harvester_image resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-image-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-image-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test the harvester_network resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-network-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-network-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test the harvester_ssh_key resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-ssh-key-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-ssh-key-resource/</guid>
      <description>These following steps must be done for every resources, for avoiding repetitions, look at the detailed instructions at the beginning of the page.
 Import a resource Generates a speculative execution plan with terraform plan command Create the resource with terraform apply command Use terraform plan again Use terraform apply again Destroy the resource with the command terraform destroy  Expected Results  The resource is well imported in the terraform.</description>
    </item>
    
    <item>
      <title>Test the harvester_virtualmachine resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-virtualmachine-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-virtualmachine-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test the harvester_volume resource (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/terraform-provider/harvester-volume-resource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/terraform-provider/harvester-volume-resource/</guid>
      <description>Refer to the harvester_ssh_key resource test steps</description>
    </item>
    
    <item>
      <title>Test zero downtime for live migration download test</title>
      <link>https://harvester.github.io/tests/manual/live-migration/zero-downtime-download-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/zero-downtime-download-test/</guid>
      <description> Connect to VM via console Start a large file download Live migrate VM to new host Verify that file download does not fail  Expected Results  Console should open VM should start to migrate File download should  </description>
    </item>
    
    <item>
      <title>Test zero downtime for live migration ping test</title>
      <link>https://harvester.github.io/tests/manual/live-migration/zero-downtime-ping-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/live-migration/zero-downtime-ping-test/</guid>
      <description> Continually ping VM Verify that ping is getting a response Live migrate VM to new host Verify that ping continues  Expected Results  Ping should get response VM should start to migrate Ping should not get any dropped packets  </description>
    </item>
    
    <item>
      <title>Timeout option for support bundle</title>
      <link>https://harvester.github.io/tests/manual/_incoming/support_bundle_timeout/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/support_bundle_timeout/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1585
Verify Items  An Timeout Option can be configured for support bundle Error message will display when reach timeout  Case: Generate support bundle but hit timeout  Install Harvester with at least 2 nodes Navigate to Advanced Settings, modify support-bundle-timeout to 2 Navigate to Support, Click Generate Support Bundle, and force shut down one of the node in the mean time. 2 mins later, the function will failed with an Error message pop up as the snapshot   </description>
    </item>
    
    <item>
      <title>toggle harvester node driver with the harvester global flag</title>
      <link>https://harvester.github.io/tests/manual/_incoming/toggle-harvester-node-driver-with-harvester-global-flag/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/toggle-harvester-node-driver-with-harvester-global-flag/</guid>
      <description>Related issue: #1465 toggle harvester node driver with the harvester global flag  Category:  Rancher Integration  Environment setup  Install rancher 2.6.3 by docker  docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps  Environment preparation as above steps Open global setting -&amp;gt; feature flag in rancher Check harvester feature flag Open cluster management -&amp;gt; Driver page Check harvester node driver Deactivate harvester feature flag Activate harvester feature flag Deactivate harvester node driver Activate harvester node driver Deactivate both harvester flag and node driver Activate harvester feature flag  Expected Results  Harvester feature flag will be enabled by default and turned on harvester node driver accordingly    If the feature flag was turned off, nothing will change to the Harvester node driver.</description>
    </item>
    
    <item>
      <title>Try to add a network with no name (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-add-network-no-name/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/negative-add-network-no-name/</guid>
      <description> Navigate to the networks page in harvester Click Create Don&amp;rsquo;t add a name Add a VLAN ID Click Create  Expected Results  You should get an error that says you need to add a name  </description>
    </item>
    
    <item>
      <title>Turn off host that is in maintenance mode (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/hosts/maintenance-mode-turn-off-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/maintenance-mode-turn-off-host/</guid>
      <description>Put host in maintenance mode Migrate VMs Wait for VMs to migrate Wait for any vms to migrate off Shut down Host  Expected Results  Host should start to go into maintenance mode Any VMs should migrate off Host should go into maintenance mode host should shut down Maintenance mode label in hosts list should go red  Known bugs https://github.com/harvester/harvester/issues/1272</description>
    </item>
    
    <item>
      <title>UI enables option to display password on login page</title>
      <link>https://harvester.github.io/tests/manual/_incoming/ui_password_show_btn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/ui_password_show_btn/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1550
Verify Items  Password field in login page can be toggle show/hide  Case: Toggle of Password field  install harvester with any nodes setup password logout then login with password toggled  </description>
    </item>
    
    <item>
      <title>Update image labels after deleting source VM</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1602-update-labels-on-image-after-vm-delete/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1602-update-labels-on-image-after-vm-delete/</guid>
      <description> Related issues: #1602 exported image can&amp;rsquo;t be deleted after vm removed  Verification Steps  create vm &amp;ldquo;vm-1&amp;rdquo; create a image &amp;ldquo;img-1&amp;rdquo; by export the volume used by vm &amp;ldquo;vm-1&amp;rdquo; delete vm &amp;ldquo;vm-1&amp;rdquo; update image &amp;ldquo;img-1&amp;rdquo; labels  Expected Results  image &amp;ldquo;img-1&amp;rdquo; will be updated  </description>
    </item>
    
    <item>
      <title>Upload Cloud Image (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/images/upload-cloud-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/upload-cloud-image/</guid>
      <description> Upload image to images page Create new vm with image using appropriate template Run VM health checks  Expected Results  Image should upload Health checks should pass  </description>
    </item>
    
    <item>
      <title>Upload image that is invalid</title>
      <link>https://harvester.github.io/tests/manual/images/negative-upload-invalid-image-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/negative-upload-invalid-image-file/</guid>
      <description>steTry to upload invalid image file to images page  Something like dmg, or tar.gzps    Expected Results  You should get an error  Known Bugs https://github.com/harvester/harvester/issues/1425</description>
    </item>
    
    <item>
      <title>Upload ISO Image</title>
      <link>https://harvester.github.io/tests/manual/images/upload-iso-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/upload-iso-image/</guid>
      <description> Upload image to images page Create new vm with image using appropriate template Run VM health checks  Expected Results  Image should upload Health checks should pass  </description>
    </item>
    
    <item>
      <title>Use a non-admin user</title>
      <link>https://harvester.github.io/tests/manual/node-driver/non-admin-user/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/non-admin-user/</guid>
      <description>create harvester user ltang, password ltang add a harvester node template Refer to the &amp;ldquo;Test Data&amp;rdquo; value setting. Use this template to create the corresponding cluster  Expected Results  The status of the created cluster shows active The status of the corresponding vm on harvester active The information displayed on rancher and harvester matches the template configuration  Test Data Harvester Node Template HARVESTER OPTIONS  Account Access Internal Harvester Username:admin Password:admin Instance Options CPUs:2 Memorys:4 Disk:40 Bus:Virtlo/SATA/SCSI Image: openSUSE-Leap-15.</description>
    </item>
    
    <item>
      <title>Use template to create cluster through virtualization management</title>
      <link>https://harvester.github.io/tests/manual/_incoming/use-template-to-create-cluster-through-virtualization-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/use-template-to-create-cluster-through-virtualization-management/</guid>
      <description>Related issue: #1620 User is unable to use template to create cluster through virtualization management  Category:  Rancher Integration  Environment setup  Install rancher 2.6.3 by docker  docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Verification Steps  Import harvester from rancher through harvester settings Access harvester from rancher virtualization management page Open Virtual Machine page Click create Check Use VM Template Select one of the template Create VM according to the template  Expected Results Access harvester from Rancher, on virtual machine page can load default three template to create VM.</description>
    </item>
    
    <item>
      <title>Validate network connectivity external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/validate-network-external-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/validate-network-external-vlan/</guid>
      <description> Create a new VM Make sure that the network is set to the external VLAN with bridge as the type Ping VM Attempt to SSH to VM  Expected Results  VM should be created You should be able to ping the VM from an external network You should be able to SSH to VM  </description>
    </item>
    
    <item>
      <title>Validate network connectivity invalid external VLAN (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/negative-network-connectivity-invalid-vlan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/negative-network-connectivity-invalid-vlan/</guid>
      <description> Create a new VM Make sure that the network is set to the external VLAN with bridge as the type and a VLAN ID that isn&amp;rsquo;t valid for your network Ping VM Attempt to SSH to VM  Expected Results  VM should be created You should not be able to ping the VM from an external network You should not be able to SSH to VM  </description>
    </item>
    
    <item>
      <title>Validate network connectivity management network (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/network/validate-network-management-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/network/validate-network-management-network/</guid>
      <description> Create a new VM Make sure that the network is set to the management network with masquerade as the type Ping VM Attempt to SSH to VM  Expected Results  VM should be created You should not be able to ping the VM from an external network You should not be able to SSH to VM  </description>
    </item>
    
    <item>
      <title>Validate QEMU agent installation</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1235-check-qemu-installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1235-check-qemu-installation/</guid>
      <description> Related issues: #1235 QEMU agent is not installed by default when creating VMs  Verification Steps  Creat openSUSE VM Start VM check for qemu-ga package Create Ubuntu VM Start VM Check for qemu-ga package  Expected Results  VMs should start Packages should be present  </description>
    </item>
    
    <item>
      <title>Validate volume shows as in use when attached (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/volumes/validate-volume-shows-in-use-while-attached/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/volumes/validate-volume-shows-in-use-while-attached/</guid>
      <description> Navigate to Volumes and check for a volume in use by a VM Verify that the state says In Use  Expected Results  State should show correctly  </description>
    </item>
    
    <item>
      <title>Verify &#34;Add Node Pool&#34;</title>
      <link>https://harvester.github.io/tests/manual/node-driver/verify-add-node-pool/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/node-driver/verify-add-node-pool/</guid>
      <description> Create a cluster of 3 nodes, One node with etcd, Control Plane, Worker, the other two with Worker The cluster is created successfully, use the command kubectl get node to view the node roles  Expected Results  The status of the created cluster shows active show the 3 created node status running in harvester&amp;rsquo;s vm list the information displayed on rancher and harvester matches the template configuration Check that the node role is correct  </description>
    </item>
    
    <item>
      <title>Verify and Configure Networking Connection (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-network-connection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-network-connection/</guid>
      <description>Provide the hostName Select management NIC bond Select the IPv4 (Automatic and Static)  Expected Results This value of hostname should be overwritten by DHCP if DHCP supplies a hostname for the system. If DHCP doesn&amp;rsquo;t offer a hostname and this value is empty, a random hostname will be generated.</description>
    </item>
    
    <item>
      <title>Verify Configuring SSH keys</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-ssh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-ssh/</guid>
      <description>Provide SSH keys while installing the Harvester. Verify user is able to login the node using that ssh key.  Expected Results User should be able to login to the node using that ssh key.</description>
    </item>
    
    <item>
      <title>Verify Configuring via HTTP URL</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-http-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-http-config/</guid>
      <description>Provide the remote Harvester config, you can find an example of the config I&amp;rsquo;m using in the deployment test plan description  Expected Results  Check that all values are taking into account  If you are using my config file, check: the node must be off after the installation the nvme and kvm modules are loaded the file /etc/test.txt exists with the correct rights the systcl values the env variable test_env should exist dns configured in /etc/resolv.</description>
    </item>
    
    <item>
      <title>Verify Enabling maintenance mode</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-enabling-maintenance-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/verify-enabling-maintenance-mode/</guid>
      <description> Navigate to the Hosts page and select the node Click Maintenance Mode  Expected Results  The existing VM should get migrated to other nodes. Verify the CRDs to see the maintenance mode is enabled.  Comments  Needs other test cases to be added If VM migration fails How does live migration work What happens if there are no schedulable resources on nodes  Check CRDs on hosts  On going into maintenance mode kubectl get virtualmachines &amp;ndash;all-namespaces   Kubectl get virtualmachines/name -o yaml  On coming out of maintenance mode kubectl get virtualmachines &amp;ndash;all-namespaces     Kubectl get virtualmachines/name -o yaml  Check that maintenance mode host isn&amp;rsquo;t schedulable  Fully provision all nodes and try to create a VM     It should fail  Migration with maintenance mode What if migration gets stuck, can you cancel VMs going to different hosts Canceling maintenance mode P1  Put in maintenance mode Check migration of VMs Check status of VMs modify filesystem on VMs Check status of host Take host out of maintenance mode Check status of host Migrate VMs back to host Check filesystem Create new VMs on host Check status of VMs      </description>
    </item>
    
    <item>
      <title>Verify network data template</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1655-network-data-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1655-network-data-template/</guid>
      <description> Related issues: #1655 When using a VM Template the Network Data in the template is not displayed  Verification Steps  Create new VM template with network data in advanced settings  network: version: 1 config: - type: physical name: interface0 subnets: - type: static address: 10.84.99.0/24 gateway: 10.84.99.254  Create new VM and select template Verify that network data is in advanced network config  Expected Results  network data should show   </description>
    </item>
    
    <item>
      <title>Verify operations like Stop, restart, pause, download YAML, generate template (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/virtual-machines/verify-operations-like-stop-restart-pause-download-yaml-generate-template/</guid>
      <description> Take an existing VM and Press the appropriate buttons for the associated operations  Stop Restart Pause Download YAML Generate Template    Expected Results  All operations should complete successfully  </description>
    </item>
    
    <item>
      <title>Verify SSH key was added from Github during install</title>
      <link>https://harvester.github.io/tests/manual/authentication/verify-github-ssh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/authentication/verify-github-ssh/</guid>
      <description> Add ssh key from Github while installing the Harvester. Login Harvester with github.  Expected Results  User should be able to logout/login successfully.  </description>
    </item>
    
    <item>
      <title>Verify that vm-force-reset-policy works</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1660-volume-unit-vm-details/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1660-volume-unit-vm-details/</guid>
      <description> Related issues: #1660 The volume unit on the vm details page is incorrect  Verification Steps  Create new .1G volume Create new VM Create with raw-image template Add opensuse base image Add .1G Volume Verify size in VM details on volume tab   Expected Results  Size should show as .1G  </description>
    </item>
    
    <item>
      <title>Verify that vm-force-reset-policy works</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1661-vm-force-reset-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1661-vm-force-reset-policy/</guid>
      <description> Related issues: #1661 vm-force-deletion-policy for vm-force-reset-policy  Environment setup Setup an airgapped harvester
 Create a 3 node harvester cluster  Verification Steps  Navigate to advanced settings and edit vm-force-reset-policy  Set reset policy to 60 Create VM Run health checks Shut down node that is running VM Check for when it starts to migrate to new Host  Expected Results  It should migrate after 60 seconds  </description>
    </item>
    
    <item>
      <title>Verify that VMs stay up when disks are evicted</title>
      <link>https://harvester.github.io/tests/manual/_incoming/1334-evict-disks-check-vms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/1334-evict-disks-check-vms/</guid>
      <description>Related issues: #1334 Volumes fail with Scheduling Failure after evicting disc on multi-disc node  Verification Steps  Created 3 node Harvester setup with ipxe example in KVM/libvirt Added formatted disk to node0 VM Created three VMs on node0 Created large files on three VMs to see where they were located with dd if=/dev/urandom of=file1.txt count=5192 bs=1M Checked Longhorn to be sure that some VMs were on new disk Deleted disk from Harvester Checked Longhorn to be sure that disk was marked for eviction Verified that VMs were still available while evicting replicas by running commands from serial console/SSH Verified that disk was removed from Longhorn and VMS were still up.</description>
    </item>
    
    <item>
      <title>Verify the external link at the bottom of the page</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-bottom-links/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/ui/verify-bottom-links/</guid>
      <description>Click all the external links available at the bottom of the page - Docs, Forums, Slack, File an issue. Click the Generate support bundle at the bottom of the page  Expected Results  The external links should take user to correct URL in new tab in the browser. The support bundle should be generated once the Generate support bundle. The progress should be shown while the bundle is getting generated.</description>
    </item>
    
    <item>
      <title>Verify the Filter on the Host page</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-filter-on-host-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/verify-filter-on-host-page/</guid>
      <description> Enter name of a host and verify the nodes get filtered out.  Expected Results  The edited name should be reflected on the host.  </description>
    </item>
    
    <item>
      <title>Verify the Harvester UI URL</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/ui/verify-url/</guid>
      <description> Navigate to the Harvester UI and verify the URL. Verify the Harvester icon on the left top corner  Expected Results  The URL should be the management ip + /dashboard redirect to login page if not login redirect to dashboard page if already login  </description>
    </item>
    
    <item>
      <title>Verify the info of the node</title>
      <link>https://harvester.github.io/tests/manual/hosts/verify-node-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/verify-node-info/</guid>
      <description> Navigate to the hosts tab and verify the following.  State Name Host IP CPU Memory Storage Size Age    Expected Results  All the data/status shown on the page should be correct.  </description>
    </item>
    
    <item>
      <title>Verify the installation confirmation screen</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-installation-confirmation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-installation-confirmation/</guid>
      <description>Verify all the details shown on the screen  Expected Results The info should reflect all the user filled data.</description>
    </item>
    
    <item>
      <title>Verify the Installer Options</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-installer-options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-installer-options/</guid>
      <description> Verify the following options available while installing the Harvester is working  Installation target Cluster token Password VIP NTP Address    Expected Results  Should show all the disks available. Verify the min and max length acceptable for cluster token. Verify the password rule  </description>
    </item>
    
    <item>
      <title>Verify the left side menu</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-left-menu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/ui/verify-left-menu/</guid>
      <description> Check all the menu at the left side of the screen. Verify the preference and logout option is available at the right top of the screen  Expected Results  The menu should have Dashboard, Hosts, Virtual machines, Volumes, Images and Advance. The Advance menu should have sub menu Templates, backups, network, SSH keys, Users, Cloud config templates, Settings. Clicking on the menu should take user to the respective pages  </description>
    </item>
    
    <item>
      <title>Verify the links which navigate to the internal pages</title>
      <link>https://harvester.github.io/tests/manual/ui/verify-internal-links/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/ui/verify-internal-links/</guid>
      <description> Click the links available on the pages like on dashboard - host, virtual machines etc Verify the events and resources tabs presents in the pages e.g. - Dashboard, Virtual machines  Expected Results  The internal link should take user to the correct page in the same tab opened in the browser  </description>
    </item>
    
    <item>
      <title>Verify the options available for image</title>
      <link>https://harvester.github.io/tests/manual/images/verify-options-available-for-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/images/verify-options-available-for-image/</guid>
      <description> Create vm with YAML using the menu option. Download Yaml Verify the downloaded Yaml file. Clone the Image  Expected Results  All user-specified fields must match what show on GUI:  Namespace Name Description URL Labels    </description>
    </item>
    
    <item>
      <title>Verify the Proxy configuration</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-proxy/</guid>
      <description>Provide a valid proxy address, verify it works after installation is complete. Provide empty proxy address.  Expected Results For empty proxy address, by default DHCP should provide the management url and it should navigate to the Harvester UI.</description>
    </item>
    
    <item>
      <title>Verify the state for Powered down node</title>
      <link>https://harvester.github.io/tests/manual/hosts/negative-verify-state-powered-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/hosts/negative-verify-state-powered-down-node/</guid>
      <description> Power down the node and check the state of the node in the cluster  Expected Results  The node state should show unavilable  </description>
    </item>
    
    <item>
      <title>VIP configured in a VLAN network should be reached</title>
      <link>https://harvester.github.io/tests/manual/_incoming/vip-configured-on-vlan-network-should-be-reached/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/vip-configured-on-vlan-network-should-be-reached/</guid>
      <description> Related issue: #1424 VIP configured in a VLAN network can not be reached  Category:  Network  Environment Setup  The network environment must have vlan network configured and also have DHCP server prepared on your testing vlan  Verification Steps  Enable virtual network with harvester-mgmt Open Network -&amp;gt; Create a virtual network Provide network name and correct vlan id  Open Route, use the default auto setting  Create a VM and use the created route SSH to harvester node Ping the IP of the created VM Create a virutal network Provide network name and correct vlan id Open Route, use the manual setting Provide the CIDR and Gateway value  Repeat step 5 - 7  Expected Results  Check the auto route vlan can be detected with running status  Check the manual route vlan can be detected with running status Check the VM can get IP based on auto or manual vlan route Check can ping VM IP from harvester node  </description>
    </item>
    
    <item>
      <title>VIP is accessibility with VLAN enabled on management port</title>
      <link>https://harvester.github.io/tests/manual/_incoming/vip_vlan_mgmtport/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/vip_vlan_mgmtport/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1722
Verify Items  VIP should be accessible when VLAN enabled on management port  Case: Single Node enables VLAN on management port  Install Harvester with single node Login to dashboard then navigate to Settings Edit vlan to enable VLAN on harvester-mgmt reboot the node after reboot, login to console Run the command should not contain any output  sudo -s kubectl get pods -A --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39; | grep harvester-network-controller-manager | xargs kubectl logs -n harvester-system | grep &amp;quot;Failed to update lock&amp;quot;   Repeat step 4-6 with 10 times, should not have any error  </description>
    </item>
    
    <item>
      <title>VIP Load balancer verification (e2e_be)</title>
      <link>https://harvester.github.io/tests/manual/deployment/verify-vip-load-balancer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/deployment/verify-vip-load-balancer/</guid>
      <description> Install Harvester on one Node  Install with VIP pulling from DHCP Verify that IP is assigned via DHCP    Add at least one additional node  Use VIP address as management address for adding node   Finish install of additional nodes Create new VM Connect to VM via web console  Expected Results  Install of all nodes should complete New nodes should show up in hosts list via web UI at VIP VMs should create Console should open  </description>
    </item>
    
    <item>
      <title>virtualmachineimages.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/virtualmachineimages.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/virtualmachineimages.harvesterhci.io/</guid>
      <description>GUI  Create an image from GUI Create another image with the same name. The operation should fail with admission webhook &amp;ldquo;validator.harvesterhci.io&amp;rdquo; denied the request: A resource with the same name exists  kube-api  Create an image from the manifest:  $ cat image.yaml --- apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: generateName: image- namespace: default spec: sourceType: download displayName: cirros-0.5.1-x86_64-disk2.img url: http://192.168.2.106/cirros-0.5.1-x86_64-disk.img $ kubectl create -f image.yml virtualmachineimage.harvesterhci.io/image-8dkbq created  Try to create an image with the same manifest:  $ kubectl create -f image.</description>
    </item>
    
    <item>
      <title>virtualmachinerestores.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/virtualmachinerestores.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/virtualmachinerestores.harvesterhci.io/</guid>
      <description>GUI  Setup a backup target Create a backup from a VM. Assume the VM name is vm-test Wait until backup is done Restore the backup to a VM, enter vm-test in the Virtual Machine Name field  kube-api $ cat restore.yaml 1 --- apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineRestore metadata: name: restore-aaaa namespace: default spec: newVM: false target: apiGroup: kubevirt.io kind: VirtualMachine name: &amp;quot;&amp;quot; virtualMachineBackupName: test $ kubectl create -f restore.yaml Expected Results GUI  The operation should fail with admission webhook &amp;ldquo;validator.</description>
    </item>
    
    <item>
      <title>virtualmachinetemplateversions.harvesterhci.io</title>
      <link>https://harvester.github.io/tests/manual/webhooks/virtualmachinetemplateversions.harvesterhci.io/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/webhooks/virtualmachinetemplateversions.harvesterhci.io/</guid>
      <description>kube-api  List default templates: $ kubectl get virtualmachinetemplateversions.harvesterhci.io -n harvester-public  GUI  Go to Advanced -&amp;gt; Templates page Create a new template and set it as the default version Try to delete the default version  Expected Results kube-api  Default templates should exist:  NAME TEMPLATE_ID VERSION AGE iso-image-base-version 1 39m raw-image-base-version 1 39m windows-iso-image-base-version 1 39m GUI  Creating a new template should succeed Deleting the default version of a template should fail with: admission webhook &amp;ldquo;validator.</description>
    </item>
    
    <item>
      <title>VM Backup with metadata</title>
      <link>https://harvester.github.io/tests/manual/_incoming/vm_backup_metadata/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/vm_backup_metadata/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/988
Verify Items  Metadata should be removed along with VM deleted Metadata should be synced after backup target switched Metadata can be used in new cluster  Case: Metadata create and delete  Install Harvester with any nodes Create an image for VM creation Setup NFS/S3 backup target Create a VM, then create a backup named backup1 File default-backup1.cfg should be exist in the backup target path &amp;lt;backup root&amp;gt;/harvester/vmbackups Delete the VM Backup backup1 File default-backup1.</description>
    </item>
    
    <item>
      <title>VM on error state</title>
      <link>https://harvester.github.io/tests/manual/_incoming/vm_on_error_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/vm_on_error_state/</guid>
      <description>Ref:
 https://github.com/harvester/harvester/issues/1446 https://github.com/harvester/harvester/issues/982  Verify Items  Error message should displayed when VM can&amp;rsquo;t be scheduled VM&amp;rsquo;s state should be changed when host is down  Case: Create a VM that no Node can host it  Install Harvester with any nodes download a image to create VM create a VM with over-commit (consider to over-provisioning feature, double or triple the host resource would be more reliable.) VM should shows Starting state, and an alart icon shows aside.</description>
    </item>
    
    <item>
      <title>VM scheduling on Specific node</title>
      <link>https://harvester.github.io/tests/manual/_incoming/vm_schedule_on_node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/vm_schedule_on_node/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1350
Verify Items  Node which is not active should not be listed in Node Scheduling list  Case: Schedule VM on the Node which is Enable Maintenance Mode  Install Harvester with at least 2 nodes Login and Navigate to Virtual Machines Create VM and Select Run VM on specific node(s)... All Active nodes should in the list Navigate to Host and pick node(s) to Enable Maintenance Mode Make sure Node(s) state changed into Maintenance Mode Repeat step 2 and 3 Picked Node(s) should not in the list Revert picked Node(s) to back to state of Active Repeat step 2 to 4  </description>
    </item>
    
    <item>
      <title>VM&#39;s CPU maximum limitation</title>
      <link>https://harvester.github.io/tests/manual/_incoming/vm_cpu_limits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/vm_cpu_limits/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1565
Verify Items  VM&amp;rsquo;s maximum CPU amount should not have limitation.  Case: Create VM with large CPU amount  Install harvester with any nodes Create image for VM creation Create a VM with vCPU over than 100 Start VM and verify lscpu shows the same amount  </description>
    </item>
    
    <item>
      <title>Volume size should be editable on derived template</title>
      <link>https://harvester.github.io/tests/manual/_incoming/derived_template_configure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harvester.github.io/tests/manual/_incoming/derived_template_configure/</guid>
      <description>Ref: https://github.com/harvester/harvester/issues/1711
Verify Items  Volume size can be changed when creating a derived template  Case: Update volume size on new template derived from exist template  Install Harvester with any Nodes Login to Dashboard Create Image for Template Creation Create Template T1 with Image Volume and additional Volume Modify Template T1 with update Volume size Volume size should be editable Click Save, then edit new version of T1 Volume size should be updated as expected  </description>
    </item>
    
  </channel>
</rss>
